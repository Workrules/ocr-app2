# dummy line
import streamlit as st
import pypdfium2 as pdfium
from azure.ai.formrecognizer import DocumentAnalysisClient
from azure.core.credentials import AzureKeyCredential
import os
import io
from PIL import Image, ImageFile
import cv2
import numpy as np
import json
from openai import OpenAI
import re
import difflib
from itertools import zip_longest
from typing import List, Tuple, Dict, Any
import gc
import time  # â˜…è¿½åŠ 

# === New: Wordå‡ºåŠ›ç”¨ ===
# dummy line
from docx import Document
from docx.shared import Cm, Pt

EMU_PER_CM = 360000.0  # 1 cm = 360,000 EMU

def to_cm(val) -> float:
    """
    python-docx ã® Lengthï¼ˆintã‚µãƒ–ã‚¯ãƒ©ã‚¹ï¼‰ã‚„ç´ ã® int ã‚’ cm(float) ã«å¤‰æ›ã€‚
    """
    try:
        # Length ãªã‚‰ .cm ã‚’å„ªå…ˆï¼ˆç’°å¢ƒã«ã‚ˆã£ã¦ã¯ç„¡ã„å ´åˆã‚‚ã‚ã‚‹ï¼‰
        return float(getattr(val, "cm"))
    except Exception:
        # int(EMU) ã®å¯èƒ½æ€§ â†’ EMU â†’ cm å¤‰æ›
        try:
            return float(val) / EMU_PER_CM
        except Exception:
            return float(val)


# ==== ç’°å¢ƒå¤‰æ•° ====
AZURE_ENDPOINT = os.getenv("AZURE_DOCINT_ENDPOINT")
AZURE_KEY = os.getenv("AZURE_DOCINT_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MODEL_NAME = os.getenv("OCR_GPT_MODEL", "gpt-5")  # gpt-5 / gpt-5-mini ãªã©
BATCH_SIZE = max(1, int(os.getenv("OCR_BATCH_PAGES", "10")))  # 10ãƒšãƒ¼ã‚¸ãšã¤å‡¦ç†

# ==== äº‹å‰ãƒã‚§ãƒƒã‚¯ ====
if not AZURE_ENDPOINT or not AZURE_KEY:
    st.error("ç’°å¢ƒå¤‰æ•° AZURE_DOCINT_ENDPOINT ã¨ AZURE_DOCINT_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# ==== ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ– ====
client = DocumentAnalysisClient(endpoint=AZURE_ENDPOINT, credential=AzureKeyCredential(AZURE_KEY))
openai_client = OpenAI(api_key=OPENAI_API_KEY)

# ==== å…±æœ‰è¾æ›¸ã®å ´æ‰€ï¼ˆç’°å¢ƒå¤‰æ•°ã§æŒ‡å®šï¼‰ ====  â˜…ã“ã“ã‚’å·®ã—æ›¿ãˆ
DICT_DIR = os.getenv("OCR_DICT_DIR", ".")  # æœªè¨­å®šãªã‚‰ã‚«ãƒ¬ãƒ³ãƒˆ
DICT_FILE = os.path.join(DICT_DIR, "ocr_char_corrections.json")
UNTRAINED_FILE = os.path.join(DICT_DIR, "untrained_confusions.json")
TRAINED_FILE = os.path.join(DICT_DIR, "trained_confusions.json")

JP_CHAR_RE = re.compile(r"^[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF]$")

# ==== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ====
def remove_red_stamp(img_pil: Image.Image) -> Image.Image:
    img = np.array(img_pil)
    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)
    lower_red1 = np.array([0, 70, 50])
    upper_red1 = np.array([10, 255, 255])
    lower_red2 = np.array([170, 70, 50])
    upper_red2 = np.array([180, 255, 255])
    mask1 = cv2.inRange(hsv, lower_red1, upper_red1)
    mask2 = cv2.inRange(hsv, lower_red2, upper_red2)
    mask = cv2.bitwise_or(mask1, mask2)
    img[mask > 0] = [255, 255, 255]
    return Image.fromarray(img)

# â˜…å®‰å…¨ãªJSON I/Oï¼ˆã‚¢ãƒˆãƒŸãƒƒã‚¯ä¿å­˜ï¼‹è»½ã„ãƒªãƒˆãƒ©ã‚¤ï¼‰
def load_json(path: str, retries: int = 3, delay: float = 0.1) -> dict:
    return _load_json_impl(path, retries, delay)

def _load_json_impl(path: str, retries: int, delay: float) -> dict:
    for _ in range(retries):
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except FileNotFoundError:
            return {}
        except (json.JSONDecodeError, PermissionError):
            time.sleep(delay)
    return {}

def save_json(obj: dict, path: str):
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
    tmp_path = f"{path}.tmp"
    with open(tmp_path, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)
        f.flush()
        os.fsync(f.fileno())
    os.replace(tmp_path, path)

def learn_charwise_with_missing(original: str, corrected: str) -> dict:
    learned = {}
    sm = difflib.SequenceMatcher(None, original, corrected)
    for tag, i1, i2, j1, j2 in sm.get_opcodes():
        if tag in ["replace", "insert"]:
            o_seg = original[i1:i2]
            c_seg = corrected[j1:j2]
            for o_char, c_char in zip_longest(o_seg, c_seg, fillvalue=""):
                if c_char and (not o_char or o_char != c_char):
                    wrong = o_char if o_char else "â–¡"
                    if JP_CHAR_RE.match(c_char) or c_char == "â–¡":
                        learned[wrong] = {"right": c_char, "count": 1}
    return learned

def update_dictionary_and_untrained(learned: dict):
    dictionary = load_json(DICT_FILE)
    for w, meta in learned.items():
        if w in dictionary:
            if dictionary[w]["right"] == meta["right"]:
                dictionary[w]["count"] += meta["count"]
            else:
                if meta["count"] > dictionary[w]["count"]:
                    dictionary[w] = meta
        else:
            dictionary[w] = meta
    save_json(dictionary, DICT_FILE)

    untrained = load_json(UNTRAINED_FILE)
    for w, meta in learned.items():
        untrained[w] = meta["right"]
    save_json(untrained, UNTRAINED_FILE)

def gpt_fix_text(text: str, dictionary: dict) -> str:
    prompt = f"""
æ¬¡ã®OCRçµæœã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«ç›´ã—ã¦ãã ã•ã„ã€‚
- æ—¥æœ¬èªã«å­˜åœ¨ã—ãªã„æ–‡å­—ã¯ã€Œâ–¡ã€ã«ã—ã¦ãã ã•ã„ã€‚
- è¾æ›¸å€™è£œã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„: {json.dumps(dictionary, ensure_ascii=False)}
- æ„å‘³ã‚’å‹æ‰‹ã«è£œå®Œã›ãšã€æœ€å°é™ã®ä¿®æ­£ã ã‘è¡Œã£ã¦ãã ã•ã„ã€‚

OCRçµæœ:
{text}
""".strip()
    try:
        # GPT-5ç³»ï¼šResponses APIï¼ˆtemperatureæœªã‚µãƒãƒ¼ãƒˆãªã®ã§æŒ‡å®šã—ãªã„ï¼‰
        resp = openai_client.responses.create(
            model=MODEL_NAME,
            input=prompt,
            text={"verbosity": "low"},
            reasoning={"effort": "minimal"}
        )
        if hasattr(resp, "output_text") and resp.output_text:
            return resp.output_text.strip()
        out_parts = []
        for item in getattr(resp, "output", []) or []:
            for part in getattr(item, "content", []) or []:
                t = getattr(part, "text", None)
                if t:
                    out_parts.append(t)
        out = "".join(out_parts).strip()
        return out or text
    except Exception as e:
        st.warning(f"GPTè£œæ­£ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰ï¼š{e}")
        return text

def render_pdf_selected_pages(pdf_bytes: bytes, indices_0based: List[int], dpi: int = 200) -> Tuple[List[Image.Image], List[int]]:
    """é¸æŠãƒšãƒ¼ã‚¸ï¼ˆ0å§‹ã¾ã‚Šï¼‰ã ã‘ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã—ã¦è¿”ã™ã€‚page_numbersã¯1å§‹ã¾ã‚Šã§è¿”ã™ã€‚"""
    imgs: List[Image.Image] = []
    nums: List[int] = []
    pdf = pdfium.PdfDocument(io.BytesIO(pdf_bytes))
    scale = dpi / 72.0
    for idx in indices_0based:
        page = pdf[idx]
        pil = page.render(scale=scale).to_pil().convert("RGB")
        imgs.append(pil)
        nums.append(idx + 1)
    return imgs, nums

def is_pdf(b: bytes) -> bool:
    return len(b) >= 5 and b[:5] == b"%PDF-"

def parse_page_spec(spec: str, max_pages: int) -> List[int]:
    """
    '1,3,5-7' ã®ã‚ˆã†ãªæŒ‡å®šã‚’0å§‹ã¾ã‚Šã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é…åˆ—ã«å¤‰æ›ã€‚
    ç¯„å›²å¤–ã¯è‡ªå‹•ã§ã‚¯ãƒªãƒƒãƒ—ã€‚é‡è¤‡ã¯æ’é™¤ã€‚æ˜‡é †ã‚½ãƒ¼ãƒˆã€‚
    """
    s = (spec or "").strip()
    if not s:
        return []
    out = set()
    parts = [p.strip() for p in s.split(",") if p.strip()]
    for p in parts:
        if "-" in p:
            a, b = p.split("-", 1)
            try:
                start = max(1, min(int(a), int(b)))
                end = min(max_pages, max(int(a), int(b)))
                for n in range(start, end + 1):
                    out.add(n - 1)
            except ValueError:
                continue
        else:
            try:
                n = int(p)
                if 1 <= n <= max_pages:
                    out.add(n - 1)
            except ValueError:
                continue
    return sorted(out)

def chunked(seq: List[int], n: int) -> List[List[int]]:
    """seqã‚’nå€‹ãšã¤ã«åˆ†å‰²ã—ã¦é †ã«è¿”ã™"""
    return [seq[i:i+n] for i in range(0, len(seq), n)]

# === New: è¡Œãƒãƒªã‚´ãƒ³ã‹ã‚‰å·¦ç«¯X/Yã‚’å–å¾— ===
def line_xy(line_obj: Any) -> Tuple[float, float]:
    """
    Azure Document Intelligenceã®Lineã®bounding_polygonã‹ã‚‰
    å·¦ç«¯xã€ä¸Šç«¯yï¼ˆå°ã•ã„ã»ã©ä¸Šï¼‰ã‚’æ¨å®šã™ã‚‹
    """
    poly = getattr(line_obj, "polygon", None) or getattr(line_obj, "bounding_polygon", None)
    if not poly:
        # v3ç³»: line.polygon ã¯ [Point], v2024: bounding_regions/â€¦å®Ÿè£…å·®ãŒã‚ã‚‹ãŸã‚ã‚¬ãƒ¼ãƒ‰
        return (0.0, 0.0)
    xs, ys = [], []
    for p in poly:
        # pã¯Point(x,y)ã‹ã€è¾æ›¸{ "x":..., "y":... } ã®æƒ³å®š
        x = getattr(p, "x", None)
        y = getattr(p, "y", None)
        if x is None and isinstance(p, dict):
            x = p.get("x", 0.0)
            y = p.get("y", 0.0)
        xs.append(float(x))
        ys.append(float(y))
    return (min(xs or [0.0]), min(ys or [0.0]))

# === New: Wordï¼ˆdocxï¼‰ã‚’ã–ã£ãã‚Šå…ƒãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã«å¯„ã›ã¦ç”Ÿæˆ ===
# dummy line
def build_docx_from_layout(pages_layout: list[dict]) -> bytes:
    doc = Document()
    section = doc.sections[0]
    section.page_width = Cm(21.0)
    section.page_height = Cm(29.7)
    section.left_margin = Cm(2.0)
    section.right_margin = Cm(2.0)
    section.top_margin = Cm(2.0)
    section.bottom_margin = Cm(2.0)

    # ã“ã“ã‚’ä¿®æ­£ï¼šæ¼”ç®—å¾Œã« .cm ã‚’å‘¼ã°ãšã€å…ˆã«å€‹åˆ¥ã« cm åŒ–ã—ã¦ã‹ã‚‰å¼•ãç®—
    page_w_cm = to_cm(section.page_width)
    left_cm = to_cm(section.left_margin)
    right_cm = to_cm(section.right_margin)
    content_width_cm = max(0.1, page_w_cm - left_cm - right_cm)

    style = doc.styles["Normal"]
    style.font.name = "Yu Gothic"
    style.font.size = Pt(11)

    for idx, page in enumerate(pages_layout, start=1):
        pw = float(page.get("page_width") or 1.0)
        ph = float(page.get("page_height") or 1.0)
        lines = page.get("lines", [])

        lines_sorted = sorted(lines, key=lambda r: (r["y"], r["x"]))
        y_thresh = ph * 0.018
        prev_y = None

        for item in lines_sorted:
            txt = item["text"]
            x = float(item["x"])
            y = float(item["y"])

            para = doc.add_paragraph()
            run = para.add_run(txt)

            # å·¦ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆï¼ˆ0ã€œ0.9*å¯ç”¨å¹…ã«ã‚¯ãƒªãƒƒãƒ—ï¼‰
            indent_cm = max(0.0, min(0.9 * content_width_cm, (x / max(pw, 1e-6)) * content_width_cm))
            para.paragraph_format.left_indent = Cm(indent_cm)

            para.paragraph_format.space_after = Pt(2)
            if prev_y is not None and (y - prev_y) > y_thresh:
                para.paragraph_format.space_before = Pt(8)
            prev_y = y

        if idx < len(pages_layout):
            doc.add_page_break()

    bio = io.BytesIO()
    doc.save(bio)
    bio.seek(0)
    return bio.read()

# ==== UI ====
st.title("ğŸ“„ Document Intelligence OCR - GPTï¼‹å°å½±é™¤å»ï¼‹æ¬ è½è£œæ­£ï¼ˆãƒšãƒ¼ã‚¸å…ˆæŒ‡å®šãƒ»10ãƒšãƒ¼ã‚¸ãƒãƒƒãƒãƒ»Wordå‡ºåŠ›ï¼‰")

dictionary = load_json(DICT_FILE)
st.sidebar.subheader("ğŸ“– ç¾åœ¨ã®è¾æ›¸")
st.sidebar.json(dictionary)
st.sidebar.markdown("### ğŸ”§ è¨ºæ–­")
st.sidebar.write({
    "AZURE_DOCINT_ENDPOINT_set": bool(AZURE_ENDPOINT),
    "AZURE_DOCINT_KEY_set": bool(AZURE_KEY),
    "OPENAI_API_KEY_set": bool(OPENAI_API_KEY),
    "MODEL": MODEL_NAME,
    "BATCH_SIZE": BATCH_SIZE,
})
# â˜…ã©ã®è¾æ›¸ã‚’å‚ç…§ã—ã¦ã„ã‚‹ã‹åˆ†ã‹ã‚‹ã‚ˆã†ã«è¿½åŠ 
st.sidebar.markdown("### ğŸ“‚ è¾æ›¸ã®å‚ç…§å…ˆ")
st.sidebar.write({
    "OCR_DICT_DIR": os.path.abspath(DICT_DIR),
    "DICT_FILE": DICT_FILE,
    "UNTRAINED_FILE": UNTRAINED_FILE,
    "TRAINED_FILE": TRAINED_FILE,
})

uploaded_file = st.file_uploader("ç”»åƒã¾ãŸã¯PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=["jpg", "jpeg", "png", "pdf"])
if not uploaded_file:
    st.info("ğŸ“‚ ã“ã“ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
    st.stop()

file_bytes = uploaded_file.read()
ImageFile.LOAD_TRUNCATED_IMAGES = True
is_input_pdf = uploaded_file.type == "application/pdf" or uploaded_file.name.lower().endswith(".pdf") or is_pdf(file_bytes)

# ==== PDFï¼šOCRå‰ã«ãƒšãƒ¼ã‚¸æŒ‡å®šãƒ•ã‚©ãƒ¼ãƒ  ====
if is_input_pdf:
    # è»½é‡ã«ãƒšãƒ¼ã‚¸æ•°ã ã‘å–å¾—
    try:
        pdf_for_count = pdfium.PdfDocument(io.BytesIO(file_bytes))
        total_pages = len(pdf_for_count)
    except Exception as e:
        st.exception(e)
        st.stop()

    with st.form("pdf_select_form"):
        st.subheader("â–¶ OCRã™ã‚‹ãƒšãƒ¼ã‚¸ã‚’å…ˆã«é¸æŠ")
        select_mode = st.radio(
            "é¸æŠæ–¹æ³•",
            options=["å…¨ãƒšãƒ¼ã‚¸", "ç¯„å›²æŒ‡å®š", "ãƒšãƒ¼ã‚¸ç•ªå·æŒ‡å®šï¼ˆä¾‹: 1,3,5-7ï¼‰"],
            index=1 if total_pages > 1 else 0,
            horizontal=True
        )
        dpi = st.slider("ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°DPIï¼ˆé«˜ã„ã»ã©ç²¾ç´°ãƒ»é‡ã„ï¼‰", min_value=72, max_value=300, value=200, step=4)

        if select_mode == "ç¯„å›²æŒ‡å®š" and total_pages > 1:
            start, end = st.slider(
                "å‡¦ç†ã™ã‚‹ãƒšãƒ¼ã‚¸ç¯„å›²ï¼ˆ1å§‹ã¾ã‚Šï¼‰",
                min_value=1, max_value=total_pages,
                value=(1, min(total_pages, 5))
            )
            chosen_indices = list(range(start - 1, end))
        elif select_mode == "ãƒšãƒ¼ã‚¸ç•ªå·æŒ‡å®šï¼ˆä¾‹: 1,3,5-7ï¼‰":
            spec = st.text_input("ãƒšãƒ¼ã‚¸ç•ªå·ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã€ç¯„å›²ã¯ãƒã‚¤ãƒ•ãƒ³ï¼‰", value="1-3" if total_pages >= 3 else "1")
            chosen_indices = parse_page_spec(spec, total_pages)
            if not chosen_indices:
                st.info("æœ‰åŠ¹ãªãƒšãƒ¼ã‚¸ç•ªå·ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ä¾‹: 1,3,5-7")
        else:
            chosen_indices = list(range(total_pages))

        submitted = st.form_submit_button("ã“ã®ãƒšãƒ¼ã‚¸ã ã‘OCRã‚’å®Ÿè¡Œ")

    if not submitted or not chosen_indices:
        st.stop()

    # ==== ã“ã“ã‹ã‚‰10ãƒšãƒ¼ã‚¸ãšã¤ã®ãƒãƒƒãƒå‡¦ç† ====
    total_to_process = len(chosen_indices)
    progress = st.progress(0.0)
    status = st.empty()
    all_corrected_texts: List[str] = []     # â˜…ãƒšãƒ¼ã‚¸è¦‹å‡ºã—ãªã—ã®é€£çµç”¨
    pages_layout: List[Dict[str, Any]] = [] # â˜…Wordãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆç”¨
    done = 0

    for batch_no, batch_indices in enumerate(chunked(chosen_indices, BATCH_SIZE), start=1):
        status.info(f"ğŸ”„ ãƒãƒƒãƒ {batch_no} / {((total_to_process - 1) // BATCH_SIZE) + 1} ã‚’å‡¦ç†ä¸­ï¼ˆãƒšãƒ¼ã‚¸: {', '.join(str(i+1) for i in batch_indices)}ï¼‰")

        # å¿…è¦ãªãƒšãƒ¼ã‚¸ã ã‘éƒ½åº¦ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã—ã¦ãƒ¡ãƒ¢ãƒªç¯€ç´„
        try:
            pages, page_numbers = render_pdf_selected_pages(file_bytes, batch_indices, dpi=dpi)
        except Exception as e:
            st.exception(e)
            st.stop()

        # ãƒãƒƒãƒå†…ãƒšãƒ¼ã‚¸ã‚’å‡¦ç†
        for page_img, page_num in zip(pages, page_numbers):
            st.write(f"## ãƒšãƒ¼ã‚¸ {page_num}")
            clean_img = remove_red_stamp(page_img)

            buf = io.BytesIO()
            clean_img.save(buf, format="PNG")
            buf.seek(0)

            with st.spinner("OCRã‚’å®Ÿè¡Œä¸­..."):
                try:
                    poller = client.begin_analyze_document("prebuilt-read", document=buf)
                    result = poller.result()
                except Exception as e:
                    st.exception(e)
                    st.stop()

            doc_page = result.pages[0] if getattr(result, "pages", None) else None
            if not doc_page:
                st.warning("OCRçµæœã«ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                done += 1
                progress.progress(done / total_to_process)
                continue

            # Azureã®è¡Œã‚’å–å¾—ï¼ˆåº§æ¨™ä»˜ãï¼‰
            azure_lines = getattr(doc_page, "lines", []) or []
            default_text = "\n".join([line.content for line in azure_lines])

            # â˜…å…±æœ‰è¾æ›¸ã‹ã‚‰æ¯å›æœ€æ–°ã‚’èª­ã¿è¾¼ã‚“ã§è£œæ­£
            dictionary = load_json(DICT_FILE)
            gpt_checked_text = gpt_fix_text(default_text, dictionary)

            # ==== ã‚¿ãƒ– ====
            tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“„ å…ƒãƒ•ã‚¡ã‚¤ãƒ«", "ğŸ–¨ï¸ OCRãƒ†ã‚­ã‚¹ãƒˆ", "ğŸ¤– GPTè£œæ­£", "âœï¸ æ‰‹ä½œæ¥­ä¿®æ­£"])
            with tab1:
                st.image(clean_img, caption=f"å…ƒãƒ•ã‚¡ã‚¤ãƒ« (ãƒšãƒ¼ã‚¸ {page_num})", use_container_width=True)
            with tab2:
                st.text_area(f"OCRãƒ†ã‚­ã‚¹ãƒˆï¼ˆãƒšãƒ¼ã‚¸ {page_num}ï¼‰", default_text, height=320, key=f"ocr_{page_num}")
            with tab3:
                st.text_area(f"GPTè£œæ­£ï¼ˆãƒšãƒ¼ã‚¸ {page_num}ï¼‰", gpt_checked_text, height=320, key=f"gpt_{page_num}")
            with tab4:
                corrected_text = st.text_area(
                    f"æ‰‹ä½œæ¥­ä¿®æ­£ï¼ˆãƒšãƒ¼ã‚¸ {page_num})ã€", gpt_checked_text, height=320, key=f"edit_{page_num}"
                )
                if st.button(f"ä¿®æ­£ã‚’ä¿å­˜ (ãƒšãƒ¼ã‚¸ {page_num})", key=f"save_{page_num}"):
                    learned = learn_charwise_with_missing(default_text, corrected_text)
                    if learned:
                        update_dictionary_and_untrained(learned)  # â˜…å…±æœ‰è¾æ›¸ã‚’æ›´æ–°
                        st.success(f"è¾æ›¸ã¨å­¦ç¿’å€™è£œã« {len(learned)} ä»¶ã‚’è¿½åŠ ã—ã¾ã—ãŸï¼")
                    else:
                        st.info("ä¿®æ­£ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                    st.rerun()

            # === (1) TXTç”¨ï¼šãƒšãƒ¼ã‚¸è¦‹å‡ºã—ãªã—ã§é€£çµ ===
            final_text_page = (corrected_text or gpt_checked_text).strip()
            all_corrected_texts.append(final_text_page)

            # === (2) Wordç”¨ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆåé›† ===
            gpt_lines = [ln for ln in (corrected_text or gpt_checked_text).splitlines()]
            lines_for_layout = []
            for i, ln in enumerate(azure_lines):
                x, y = line_xy(ln)
                text_for_line = gpt_lines[i] if i < len(gpt_lines) else ln.content
                lines_for_layout.append({"text": text_for_line, "x": x, "y": y})

            page_layout_info = {
                "page_width": getattr(doc_page, "width", None) or 1.0,
                "page_height": getattr(doc_page, "height", None) or 1.0,
                "unit": getattr(doc_page, "unit", None) or "pixel",
                "lines": lines_for_layout
            }
            pages_layout.append(page_layout_info)

            # é€²æ—æ›´æ–°
            done += 1
            progress.progress(done / total_to_process)

        # ãƒãƒƒãƒçµ‚äº†ã”ã¨ã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        del pages
        del page_numbers
        gc.collect()

    status.success("âœ… ã™ã¹ã¦ã®ãƒšãƒ¼ã‚¸ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    # ==== ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒšãƒ¼ã‚¸è¦‹å‡ºã—ãªã—TXT / Wordï¼‰ ====
    if all_corrected_texts:
        joined_txt = "\n\n".join(all_corrected_texts)
        st.download_button(
            "ğŸ“¥ è£œæ­£ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆTXT, ãƒšãƒ¼ã‚¸è¦‹å‡ºã—ãªã—ï¼‰",
            data=joined_txt.encode("utf-8"),
            file_name="ocr_corrected.txt",
            mime="text/plain"
        )

    if pages_layout:
        try:
            docx_bytes = build_docx_from_layout(pages_layout)
            st.download_button(
                "ğŸ“¥ Wordï¼ˆ.docxï¼‰ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¿‘ä¼¼ï¼‰",
                data=docx_bytes,
                file_name="ocr_layout.docx",
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
            )
        except Exception as e:
            st.warning(f"Wordå‡ºåŠ›ã«å¤±æ•—ã—ã¾ã—ãŸï¼š{e}")

else:
    # ==== ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ï¼šãã®ã¾ã¾1ãƒšãƒ¼ã‚¸å‡¦ç† ====
    try:
        try:
            img = Image.open(io.BytesIO(file_bytes)).convert("RGB")
        except Exception:
            arr = np.frombuffer(file_bytes, dtype=np.uint8)
            bgr = cv2.imdecode(arr, cv2.IMREAD_COLOR)
            if bgr is None:
                raise ValueError("ç”»åƒã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆJPG/PNG/PDFã®ã¿å¯¾å¿œï¼‰ã€‚")
            img = Image.fromarray(cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB))
        pages = [img]
        page_numbers = [1]
    except Exception as e:
        st.exception(e)
        st.stop()

    all_corrected_texts: List[str] = []
    pages_layout: List[Dict[str, Any]] = []

    for page_img, page_num in zip(pages, page_numbers):
        st.write(f"## ãƒšãƒ¼ã‚¸ {page_num}")
        clean_img = remove_red_stamp(page_img)

        buf = io.BytesIO()
        clean_img.save(buf, format="PNG")
        buf.seek(0)

        with st.spinner("OCRã‚’å®Ÿè¡Œä¸­..."):
            try:
                poller = client.begin_analyze_document("prebuilt-read", document=buf)
                result = poller.result()
            except Exception as e:
                st.exception(e)
                st.stop()

        doc_page = result.pages[0] if getattr(result, "pages", None) else None
        if not doc_page:
            st.warning("OCRçµæœã«ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            continue

        azure_lines = getattr(doc_page, "lines", []) or []
        default_text = "\n".join([line.content for line in azure_lines])

        # â˜…å…±æœ‰è¾æ›¸ã‚’ä½¿ç”¨
        dictionary = load_json(DICT_FILE)
        gpt_checked_text = gpt_fix_text(default_text, dictionary)

        tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“„ å…ƒãƒ•ã‚¡ã‚¤ãƒ«", "ğŸ–¨ï¸ OCRãƒ†ã‚­ã‚¹ãƒˆ", "ğŸ¤– GPTè£œæ­£", "âœï¸ æ‰‹ä½œæ¥­ä¿®æ­£"])
        with tab1:
            st.image(clean_img, caption=f"å…ƒãƒ•ã‚¡ã‚¤ãƒ« (ãƒšãƒ¼ã‚¸ {page_num})", use_container_width=True)
        with tab2:
            st.text_area(f"OCRãƒ†ã‚­ã‚¹ãƒˆï¼ˆãƒšãƒ¼ã‚¸ {page_num}ï¼‰", default_text, height=320, key=f"ocr_{page_num}")
        with tab3:
            st.text_area(f"GPTè£œæ­£ï¼ˆãƒšãƒ¼ã‚¸ {page_num}ï¼‰", gpt_checked_text, height=320, key=f"gpt_{page_num}")
        with tab4:
            corrected_text = st.text_area(
                f"æ‰‹ä½œæ¥­ä¿®æ­£ï¼ˆãƒšãƒ¼ã‚¸ {page_num}ï¼‰", gpt_checked_text, height=320, key=f"edit_{page_num}"
            )
            if st.button(f"ä¿®æ­£ã‚’ä¿å­˜ (ãƒšãƒ¼ã‚¸ {page_num})", key=f"save_{page_num}"):
                learned = learn_charwise_with_missing(default_text, corrected_text)
                if learned:
                    update_dictionary_and_untrained(learned)  # â˜…å…±æœ‰è¾æ›¸ã‚’æ›´æ–°
                    st.success(f"è¾æ›¸ã¨å­¦ç¿’å€™è£œã« {len(learned)} ä»¶ã‚’è¿½åŠ ã—ã¾ã—ãŸï¼")
                else:
                    st.info("ä¿®æ­£ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                st.rerun()

        # TXTï¼ˆãƒšãƒ¼ã‚¸è¦‹å‡ºã—ãªã—ï¼‰
        final_text_page = (corrected_text or gpt_checked_text).strip()
        all_corrected_texts.append(final_text_page)

        # Wordãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæƒ…å ±
        gpt_lines = [ln for ln in (corrected_text or gpt_checked_text).splitlines()]
        lines_for_layout = []
        for i, ln in enumerate(azure_lines):
            x, y = line_xy(ln)
            text_for_line = gpt_lines[i] if i < len(gpt_lines) else ln.content
            lines_for_layout.append({"text": text_for_line, "x": x, "y": y})

        page_layout_info = {
            "page_width": getattr(doc_page, "width", None) or 1.0,
            "page_height": getattr(doc_page, "height", None) or 1.0,
            "unit": getattr(doc_page, "unit", None) or "pixel",
            "lines": lines_for_layout
        }
        pages_layout.append(page_layout_info)

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆTXT / Wordï¼‰
    if all_corrected_texts:
        joined_txt = "\n\n".join(all_corrected_texts)
        st.download_button(
            "ğŸ“¥ è£œæ­£ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆTXT, ãƒšãƒ¼ã‚¸è¦‹å‡ºã—ãªã—ï¼‰",
            data=joined_txt.encode("utf-8"),
            file_name="ocr_corrected.txt",
            mime="text/plain"
        )

    if pages_layout:
        try:
            docx_bytes = build_docx_from_layout(pages_layout)
            st.download_button(
                "ğŸ“¥ Wordï¼ˆ.docxï¼‰ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¿‘ä¼¼ï¼‰",
                data=docx_bytes,
                file_name="ocr_layout.docx",
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
            )
        except Exception as e:
            st.warning(f"Wordå‡ºåŠ›ã«å¤±æ•—ã—ã¾ã—ãŸï¼š{e}")
