# dummy line
import streamlit as st
import os
import io
import json
import time
import gc
import re
from itertools import zip_longest
from typing import List, Tuple, Dict, Any

import numpy as np
import cv2
from PIL import Image, ImageFile
import pypdfium2 as pdfium

from azure.ai.formrecognizer import DocumentAnalysisClient
from azure.core.credentials import AzureKeyCredential

from openai import OpenAI

# ===== Wordå‡ºåŠ›ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¿‘ä¼¼ï¼‰ =====
from docx import Document
from docx.shared import Cm, Pt

# ===================== åŸºæœ¬è¨­å®š =====================
# Azure Document Intelligence / OpenAI
AZURE_DOCINT_ENDPOINT = os.getenv("AZURE_DOCINT_ENDPOINT") or st.secrets.get("AZURE_DOCINT_ENDPOINT")
AZURE_DOCINT_KEY = os.getenv("AZURE_DOCINT_KEY") or st.secrets.get("AZURE_DOCINT_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY")
MODEL_NAME = os.getenv("OCR_GPT_MODEL") or st.secrets.get("OCR_GPT_MODEL", "gpt-5")
BATCH_SIZE_DEFAULT = max(1, int(os.getenv("OCR_BATCH_PAGES") or st.secrets.get("OCR_BATCH_PAGES", "10")))

if not AZURE_DOCINT_ENDPOINT or not AZURE_DOCINT_KEY:
    st.error("ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯Secretsã§ AZURE_DOCINT_ENDPOINT / AZURE_DOCINT_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
client = DocumentAnalysisClient(endpoint=AZURE_DOCINT_ENDPOINT, credential=AzureKeyCredential(AZURE_DOCINT_KEY))
openai_client = OpenAI(api_key=OPENAI_API_KEY)

# æ–‡å­—åˆ¤å®š
JP_CHAR_RE = re.compile(r"^[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF]$")

# ===================== ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸é¸æŠï¼ˆlocal / azureblobï¼‰ =====================
STORAGE_BACKEND = os.getenv("OCR_DICT_BACKEND") or st.secrets.get("OCR_DICT_BACKEND", "local")

if STORAGE_BACKEND == "azureblob":
    # Azure Blob Storage ã‚’ä½¿ç”¨ï¼ˆæœ¬ç•ªWebæ¨å¥¨ï¼‰
    try:
        from azure.storage.blob import BlobServiceClient, ContentSettings
        from azure.core.exceptions import ResourceNotFoundError, ResourceExistsError
    except Exception as e:
        st.error(f"azure-storage-blob ãŒå¿…è¦ã§ã™ã€‚requirements.txt ã«è¿½åŠ ã—ã¦ãã ã•ã„ã€‚è©³ç´°: {e}")
        st.stop()

    AZURE_BLOB_CONN_STR = os.getenv("AZURE_BLOB_CONN_STR") or st.secrets.get("AZURE_BLOB_CONN_STR")
    OCR_DICT_CONTAINER = os.getenv("OCR_DICT_CONTAINER") or st.secrets.get("OCR_DICT_CONTAINER", "ocr-shared-dict")

    if not AZURE_BLOB_CONN_STR:
        st.error("Secrets/ç’°å¢ƒå¤‰æ•° AZURE_BLOB_CONN_STR ã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼ˆBlobæ¥ç¶šæ–‡å­—åˆ—ï¼‰ã€‚")
        st.stop()

    _blob_service = BlobServiceClient.from_connection_string(AZURE_BLOB_CONN_STR)
    _container = _blob_service.get_container_client(OCR_DICT_CONTAINER)
    try:
        _container.create_container()
    except ResourceExistsError:
        pass  # æ—¢å­˜ãªã‚‰OK

    # Blobåï¼ˆå›ºå®šï¼‰
    DICT_FILE = "ocr_char_corrections.json"
    UNTRAINED_FILE = "untrained_confusions.json"
    TRAINED_FILE = "trained_confusions.json"

    # Blob I/O
    def _load_json_blob(blob_name: str) -> dict:
        try:
            data = _container.download_blob(blob_name).readall()
            return json.loads(data.decode("utf-8"))
        except ResourceNotFoundError:
            return {}
        except Exception:
            return {}

    def _save_json_blob(obj: dict, blob_name: str):
        b = json.dumps(obj, ensure_ascii=False, indent=2).encode("utf-8")
        _container.upload_blob(
            name=blob_name,
            data=b,
            overwrite=True,
            content_settings=ContentSettings(content_type="application/json; charset=utf-8"),
        )

    def load_json_any(key: str) -> dict:
        return _load_json_blob(key)

    def save_json_any(obj: dict, key: str):
        _save_json_blob(obj, key)

elif STORAGE_BACKEND == "local":
    # ãƒ­ãƒ¼ã‚«ãƒ«JSONï¼ˆé–‹ç™ºãƒ»æ¤œè¨¼å‘ã‘ï¼‰
    DICT_DIR = os.getenv("OCR_DICT_DIR") or st.secrets.get("OCR_DICT_DIR", ".")
    DICT_FILE = os.path.join(DICT_DIR, "ocr_char_corrections.json")
    UNTRAINED_FILE = os.path.join(DICT_DIR, "untrained_confusions.json")
    TRAINED_FILE = os.path.join(DICT_DIR, "trained_confusions.json")

    def _load_json_local(path: str, retries: int = 3, delay: float = 0.1) -> dict:
        for _ in range(retries):
            try:
                with open(path, "r", encoding="utf-8") as f:
                    return json.load(f)
            except FileNotFoundError:
                return {}
            except (json.JSONDecodeError, PermissionError):
                time.sleep(delay)
        return {}

    def _save_json_local(obj: dict, path: str):
        os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
        tmp_path = f"{path}.tmp"
        with open(tmp_path, "w", encoding="utf-8") as f:
            json.dump(obj, f, ensure_ascii=False, indent=2)
            f.flush(); os.fsync(f.fileno())
        os.replace(tmp_path, path)

    def load_json_any(key: str) -> dict:
        return _load_json_local(key)

    def save_json_any(obj: dict, key: str):
        _save_json_local(obj, key)

else:
    st.error(f"æœªçŸ¥ã® OCR_DICT_BACKEND: {STORAGE_BACKEND}ï¼ˆlocal / azureblob ã®ã¿å¯¾å¿œï¼‰")
    st.stop()

# ===================== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =====================
def remove_red_stamp(img_pil: Image.Image) -> Image.Image:
    """èµ¤ã„å°å½±ã‚’ç™½ã«é£›ã°ã™ç°¡æ˜“ãƒ•ã‚£ãƒ«ã‚¿"""
    img = np.array(img_pil)
    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)
    lower_red1 = np.array([0, 70, 50]); upper_red1 = np.array([10, 255, 255])
    lower_red2 = np.array([170, 70, 50]); upper_red2 = np.array([180, 255, 255])
    mask = cv2.bitwise_or(cv2.inRange(hsv, lower_red1, upper_red1),
                          cv2.inRange(hsv, lower_red2, upper_red2))
    img[mask > 0] = [255, 255, 255]
    return Image.fromarray(img)

def learn_charwise_with_missing(original: str, corrected: str) -> dict:
    """æ–‡å­—å˜ä½ã®å·®åˆ†ã‹ã‚‰ã€Œèª¤â†’æ­£ã€ã‚’å­¦ç¿’ï¼ˆæ¬ è½ã¯ 'â–¡' ã¨ã—ã¦æ‰±ã†ï¼‰"""
    learned = {}
    import difflib
    sm = difflib.SequenceMatcher(None, original, corrected)
    for tag, i1, i2, j1, j2 in sm.get_opcodes():
        if tag in ["replace", "insert"]:
            o_seg, c_seg = original[i1:i2], corrected[j1:j2]
            for o_char, c_char in zip_longest(o_seg, c_seg, fillvalue=""):
                if c_char and (not o_char or o_char != c_char):
                    wrong = o_char if o_char else "â–¡"
                    if JP_CHAR_RE.match(c_char) or c_char == "â–¡":
                        learned[wrong] = {"right": c_char, "count": 1}
    return learned

def update_dictionary_and_untrained(learned: dict):
    # ãƒ¡ã‚¤ãƒ³è¾æ›¸
    dictionary = load_json_any(DICT_FILE)
    for w, meta in learned.items():
        if w in dictionary:
            if dictionary[w]["right"] == meta["right"]:
                dictionary[w]["count"] += meta["count"]
            else:
                if meta["count"] > dictionary[w]["count"]:
                    dictionary[w] = meta
        else:
            dictionary[w] = meta
    save_json_any(dictionary, DICT_FILE)
    # æœªå­¦ç¿’å€™è£œ
    untrained = load_json_any(UNTRAINED_FILE)
    for w, meta in learned.items():
        untrained[w] = meta["right"]
    save_json_any(untrained, UNTRAINED_FILE)

def gpt_fix_text(text: str, dictionary: dict) -> str:
    """GPTã§æœ€å°é™ã«æ•´å½¢ï¼ˆtemperatureç­‰ã¯æœªæŒ‡å®šï¼šgpt-5æ—¢å®šå€¤ã§å®Ÿè¡Œï¼‰"""
    prompt = f"""
æ¬¡ã®OCRçµæœã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«ç›´ã—ã¦ãã ã•ã„ã€‚
- æ—¥æœ¬èªã«å­˜åœ¨ã—ãªã„æ–‡å­—ã¯ã€Œâ–¡ã€ã«ã—ã¦ãã ã•ã„ã€‚
- è¾æ›¸å€™è£œã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„: {json.dumps(dictionary, ensure_ascii=False)}
- æ„å‘³ã‚’å‹æ‰‹ã«è£œå®Œã›ãšã€æœ€å°é™ã®ä¿®æ­£ã ã‘è¡Œã£ã¦ãã ã•ã„ã€‚

OCRçµæœ:
{text}
""".strip()
    try:
        resp = openai_client.responses.create(
            model=MODEL_NAME,
            input=prompt,
            text={"verbosity": "low"},
            reasoning={"effort": "minimal"}
        )
        if hasattr(resp, "output_text") and resp.output_text:
            return resp.output_text.strip()
        out_parts = []
        for item in getattr(resp, "output", []) or []:
            for part in getattr(item, "content", []) or []:
                t = getattr(part, "text", None)
                if t:
                    out_parts.append(t)
        out = "".join(out_parts).strip()
        return out or text
    except Exception as e:
        st.warning(f"GPTè£œæ­£ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰ï¼š{e}")
        return text

# ========== PDFãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ãƒ»ãƒšãƒ¼ã‚¸æŒ‡å®š ==========
def is_pdf(b: bytes) -> bool:
    return len(b) >= 5 and b[:5] == b"%PDF-"

def render_pdf_selected_pages(pdf_bytes: bytes, indices_0based: List[int], dpi: int = 200) -> Tuple[List[Image.Image], List[int]]:
    """é¸æŠãƒšãƒ¼ã‚¸ï¼ˆ0å§‹ã¾ã‚Šï¼‰ã ã‘ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã—ã¦è¿”ã™ã€‚æˆ»ã‚Šã®page_numbersã¯1å§‹ã¾ã‚Šã€‚"""
    imgs: List[Image.Image] = []
    nums: List[int] = []
    pdf = pdfium.PdfDocument(io.BytesIO(pdf_bytes))
    scale = dpi / 72.0
    for idx in indices_0based:
        page = pdf[idx]
        pil = page.render(scale=scale).to_pil().convert("RGB")
        imgs.append(pil); nums.append(idx + 1)
    return imgs, nums

def parse_page_spec(spec: str, max_pages: int) -> List[int]:
    """'1,3,5-7' â†’ 0å§‹ã¾ã‚Šã®æ˜‡é †ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é…åˆ—ï¼ˆç¯„å›²å¤–ã‚¯ãƒªãƒƒãƒ—ãƒ»é‡è¤‡æ’é™¤ï¼‰"""
    s = (spec or "").strip()
    if not s:
        return []
    out = set()
    parts = [p.strip() for p in s.split(",") if p.strip()]
    for p in parts:
        if "-" in p:
            a, b = p.split("-", 1)
            try:
                start = max(1, min(int(a), int(b)))
                end = min(max_pages, max(int(a), int(b)))
                for n in range(start, end + 1):
                    out.add(n - 1)
            except ValueError:
                continue
        else:
            try:
                n = int(p)
                if 1 <= n <= max_pages:
                    out.add(n - 1)
            except ValueError:
                continue
    return sorted(out)

def chunked(seq: List[int], n: int) -> List[List[int]]:
    return [seq[i:i+n] for i in range(0, len(seq), n)]

# ========== Azureè¡Œã®åº§æ¨™ï¼ˆå·¦x/ä¸Šyï¼‰ ==========
def line_xy(line_obj: Any) -> Tuple[float, float]:
    poly = getattr(line_obj, "polygon", None) or getattr(line_obj, "bounding_polygon", None)
    if not poly:
        return (0.0, 0.0)
    xs, ys = [], []
    for p in poly:
        x = getattr(p, "x", None); y = getattr(p, "y", None)
        if x is None and isinstance(p, dict):
            x = p.get("x", 0.0); y = p.get("y", 0.0)
        xs.append(float(x)); ys.append(float(y))
    return (min(xs or [0.0]), min(ys or [0.0]))

# ========== Wordï¼ˆdocxï¼‰ç”Ÿæˆï¼ˆå·¦ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆè¿‘ä¼¼ã®ã¿ï¼‰ ==========
EMU_PER_CM = 360000.0
def to_cm(val) -> float:
    try:
        return float(getattr(val, "cm"))
    except Exception:
        try:
            return float(val) / EMU_PER_CM
        except Exception:
            return float(val)

def build_docx_from_layout(pages_layout: List[Dict[str, Any]]) -> bytes:
    doc = Document()
    section = doc.sections[0]
    section.page_width = Cm(21.0); section.page_height = Cm(29.7)
    section.left_margin = Cm(2.0); section.right_margin = Cm(2.0)
    section.top_margin = Cm(2.0); section.bottom_margin = Cm(2.0)

    page_w_cm = to_cm(section.page_width)
    left_cm = to_cm(section.left_margin); right_cm = to_cm(section.right_margin)
    content_width_cm = max(0.1, page_w_cm - left_cm - right_cm)

    style = doc.styles["Normal"]; style.font.name = "Yu Gothic"; style.font.size = Pt(11)

    for idx, page in enumerate(pages_layout, start=1):
        pw = float(page.get("page_width") or 1.0)
        ph = float(page.get("page_height") or 1.0)
        lines = page.get("lines", [])
        lines_sorted = sorted(lines, key=lambda r: (r["y"], r["x"]))
        y_thresh = ph * 0.018
        prev_y = None

        for item in lines_sorted:
            txt = item["text"]; x = float(item["x"]); y = float(item["y"])
            para = doc.add_paragraph(); para.add_run(txt)
            indent_cm = max(0.0, min(0.9 * content_width_cm, (x / max(pw, 1e-6)) * content_width_cm))
            para.paragraph_format.left_indent = Cm(indent_cm)
            para.paragraph_format.space_after = Pt(2)
            if prev_y is not None and (y - prev_y) > y_thresh:
                para.paragraph_format.space_before = Pt(8)
            prev_y = y

        if idx < len(pages_layout):
            doc.add_page_break()

    bio = io.BytesIO(); doc.save(bio); bio.seek(0)
    return bio.read()

# ===================== UI =====================
st.title("ğŸ“„ Document Intelligence OCR - Webï¼ˆAzure Blobè¾æ›¸ï¼‰/ ãƒšãƒ¼ã‚¸æŒ‡å®š / 10ãƒšãƒ¼ã‚¸ãƒãƒƒãƒ / GPTè£œæ­£ / Wordå‡ºåŠ› / ç”»é¢ä¿æŒ")

# è¨ºæ–­
st.sidebar.markdown("### ğŸ”§ ç’°å¢ƒ")
st.sidebar.write({
    "AZURE_DOCINT_ENDPOINT_set": bool(AZURE_DOCINT_ENDPOINT),
    "AZURE_DOCINT_KEY_set": bool(AZURE_DOCINT_KEY),
    "OPENAI_API_KEY_set": bool(OPENAI_API_KEY),
    "MODEL": MODEL_NAME,
    "BATCH_SIZE(default)": BATCH_SIZE_DEFAULT,
})

st.sidebar.markdown("### ğŸ“‚ è¾æ›¸ã®å‚ç…§å…ˆ")
if STORAGE_BACKEND == "azureblob":
    st.sidebar.write({
        "OCR_DICT_BACKEND": "azureblob",
        "container": os.getenv("OCR_DICT_CONTAINER") or st.secrets.get("OCR_DICT_CONTAINER", "ocr-shared-dict"),
        "DICT_BLOB": DICT_FILE,
        "UNTRAINED_BLOB": UNTRAINED_FILE,
        "TRAINED_BLOB": TRAINED_FILE,
    })
else:
    st.sidebar.write({
        "OCR_DICT_BACKEND": "local",
        "OCR_DICT_DIR": os.path.abspath(os.getenv("OCR_DICT_DIR") or st.secrets.get("OCR_DICT_DIR", ".")),
        "DICT_FILE": os.path.abspath(DICT_FILE),
        "UNTRAINED_FILE": os.path.abspath(UNTRAINED_FILE),
        "TRAINED_FILE": os.path.abspath(TRAINED_FILE),
    })

# ãƒ‡ãƒãƒƒã‚°UI
st.sidebar.markdown("### ğŸ›  ãƒ‡ãƒãƒƒã‚°")
skip_gpt = st.sidebar.checkbox("GPTè£œæ­£ã‚’ã‚¹ã‚­ãƒƒãƒ—", value=False)
ocr_timeout = st.sidebar.slider("OCRã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰", 10, 180, 60, step=5)
batch_size_override = st.sidebar.number_input("ãƒãƒƒãƒã‚µã‚¤ã‚ºä¸Šæ›¸ã", 1, 20, value=BATCH_SIZE_DEFAULT)

# ç¾åœ¨ã®è¾æ›¸ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
dictionary_preview = load_json_any(DICT_FILE)
st.sidebar.subheader("ğŸ“– ç¾åœ¨ã®è¾æ›¸ï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰")
st.sidebar.json(dictionary_preview)

# ãƒ•ã‚¡ã‚¤ãƒ«å…¥åŠ›
uploaded_file = st.file_uploader("ç”»åƒã¾ãŸã¯PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=["jpg", "jpeg", "png", "pdf"])
if not uploaded_file:
    st.info("ğŸ“‚ ã“ã“ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
    st.stop()

file_bytes = uploaded_file.read()
ImageFile.LOAD_TRUNCATED_IMAGES = True
is_input_pdf = uploaded_file.type == "application/pdf" or uploaded_file.name.lower().endswith(".pdf") or is_pdf(file_bytes)

# ===================== PDFï¼šOCRå‰ã«ãƒšãƒ¼ã‚¸æŒ‡å®š =====================
if is_input_pdf:
    try:
        pdf_for_count = pdfium.PdfDocument(io.BytesIO(file_bytes))
        total_pages = len(pdf_for_count)
    except Exception as e:
        st.exception(e); st.stop()

    with st.form("pdf_select_form"):
        st.subheader("â–¶ OCRã™ã‚‹ãƒšãƒ¼ã‚¸ã‚’å…ˆã«é¸æŠ")
        select_mode = st.radio(
            "é¸æŠæ–¹æ³•",
            options=["å…¨ãƒšãƒ¼ã‚¸", "ç¯„å›²æŒ‡å®š", "ãƒšãƒ¼ã‚¸ç•ªå·æŒ‡å®šï¼ˆä¾‹: 1,3,5-7ï¼‰"],
            index=1 if total_pages > 1 else 0,
            horizontal=True
        )
        dpi = st.slider("ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°DPIï¼ˆé«˜ã„ã»ã©ç²¾ç´°ãƒ»é‡ã„ï¼‰", 72, 300, 200, step=4)

        if select_mode == "ç¯„å›²æŒ‡å®š" and total_pages > 1:
            start, end = st.slider("å‡¦ç†ã™ã‚‹ãƒšãƒ¼ã‚¸ç¯„å›²ï¼ˆ1å§‹ã¾ã‚Šï¼‰", 1, total_pages, (1, min(total_pages, 5)))
            chosen_indices = list(range(start - 1, end))
        elif select_mode == "ãƒšãƒ¼ã‚¸ç•ªå·æŒ‡å®šï¼ˆä¾‹: 1,3,5-7ï¼‰":
            spec = st.text_input("ãƒšãƒ¼ã‚¸ç•ªå·ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã€ç¯„å›²ã¯ãƒã‚¤ãƒ•ãƒ³ï¼‰", value="1-3" if total_pages >= 3 else "1")
            chosen_indices = parse_page_spec(spec, total_pages)
            if not chosen_indices:
                st.info("æœ‰åŠ¹ãªãƒšãƒ¼ã‚¸ç•ªå·ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ä¾‹: 1,3,5-7")
        else:
            chosen_indices = list(range(total_pages))

        submitted = st.form_submit_button("ã“ã®ãƒšãƒ¼ã‚¸ã ã‘OCRã‚’å®Ÿè¡Œ")

    if not submitted or not chosen_indices:
        st.stop()

    EFFECTIVE_BATCH = int(batch_size_override) if batch_size_override else BATCH_SIZE_DEFAULT

    total_to_process = len(chosen_indices)
    progress = st.progress(0.0)
    status = st.empty()
    all_corrected_texts: List[str] = []      # TXTç”¨ï¼ˆãƒšãƒ¼ã‚¸è¦‹å‡ºã—ãªã—ï¼‰
    pages_layout: List[Dict[str, Any]] = []  # Wordç”¨
    done = 0

    for batch_no, batch_indices in enumerate(chunked(chosen_indices, EFFECTIVE_BATCH), start=1):
        status.info(f"ğŸ”„ ãƒãƒƒãƒ {batch_no} / {((total_to_process - 1) // EFFECTIVE_BATCH) + 1} ã‚’å‡¦ç†ä¸­ï¼ˆãƒšãƒ¼ã‚¸: {', '.join(str(i+1) for i in batch_indices)}ï¼‰")
        try:
            pages, page_numbers = render_pdf_selected_pages(file_bytes, batch_indices, dpi=dpi)
        except Exception as e:
            st.exception(e); st.stop()

        for page_img, page_num in zip(pages, page_numbers):
            st.write(f"## ãƒšãƒ¼ã‚¸ {page_num}")
            clean_img = remove_red_stamp(page_img)

            buf = io.BytesIO(); clean_img.save(buf, format="PNG"); buf.seek(0)

            with st.spinner("OCRã‚’å®Ÿè¡Œä¸­..."):
                t0 = time.perf_counter()
                try:
                    poller = client.begin_analyze_document("prebuilt-read", document=buf)
                    result = poller.result(timeout=float(ocr_timeout))
                except Exception as e:
                    st.error(f"OCRãŒ{ocr_timeout}ç§’ä»¥å†…ã«å®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸ / å¤±æ•—ã—ã¾ã—ãŸï¼š{e}")
                    st.caption(f"OCRå®Ÿè¡Œæ™‚é–“: {time.perf_counter() - t0:.1f}s")
                    continue
                st.caption(f"OCRå®Ÿè¡Œæ™‚é–“: {time.perf_counter() - t0:.1f}s")

            doc_page = result.pages[0] if getattr(result, "pages", None) else None
            if not doc_page:
                st.warning("OCRçµæœã«ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                done += 1; progress.progress(done / total_to_process)
                continue

            azure_lines = getattr(doc_page, "lines", []) or []
            default_text = "\n".join([line.content for line in azure_lines])

            # å…±æœ‰è¾æ›¸ã‚’æ¯å›æœ€æ–°ã§èª­ã¿ã¤ã¤ã€å¿…è¦ã«å¿œã˜ã¦GPTè£œæ­£
            dictionary = load_json_any(DICT_FILE)
            gpt_checked_text = default_text if skip_gpt else gpt_fix_text(default_text, dictionary)

            # --- ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚­ãƒ¼ ---
            ocr_key = f"ocr_{page_num}"
            gpt_key = f"gpt_{page_num}"
            edit_key = f"edit_{page_num}"

            # åˆæœŸåŒ–ï¼ˆåˆå›ã ã‘ï¼‰
            if ocr_key not in st.session_state:
                st.session_state[ocr_key] = default_text
            if gpt_key not in st.session_state:
                st.session_state[gpt_key] = gpt_checked_text
            if edit_key not in st.session_state:
                st.session_state[edit_key] = gpt_checked_text

            # ãƒ“ãƒ¥ãƒ¼ã‚¿ãƒ–
            tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“„ å…ƒãƒ•ã‚¡ã‚¤ãƒ«", "ğŸ–¨ï¸ OCRãƒ†ã‚­ã‚¹ãƒˆ", "ğŸ¤– GPTè£œæ­£", "âœï¸ æ‰‹ä½œæ¥­ä¿®æ­£"])
            with tab1:
                st.image(clean_img, caption=f"å…ƒãƒ•ã‚¡ã‚¤ãƒ« (ãƒšãƒ¼ã‚¸ {page_num})", use_container_width=True)
            with tab2:
                st.text_area(
                    f"OCRãƒ†ã‚­ã‚¹ãƒˆï¼ˆãƒšãƒ¼ã‚¸ {page_num}ï¼‰",
                    value=st.session_state.get(ocr_key, default_text),
                    height=320,
                    key=ocr_key
                )
            with tab3:
                st.text_area(
                    f"GPTè£œæ­£ï¼ˆãƒšãƒ¼ã‚¸ {page_num}ï¼‰",
                    value=st.session_state.get(gpt_key, gpt_checked_text),
                    height=320,
                    key=gpt_key
                )
            with tab4:
                st.text_area(
                    f"æ‰‹ä½œæ¥­ä¿®æ­£ï¼ˆãƒšãƒ¼ã‚¸ {page_num}ï¼‰",
                    value=st.session_state.get(edit_key, gpt_checked_text),
                    height=320,
                    key=edit_key
                )
                if st.button(f"ä¿®æ­£ã‚’ä¿å­˜ (ãƒšãƒ¼ã‚¸ {page_num})", key=f"save_{page_num}"):
                    corrected_text_current = st.session_state.get(edit_key, gpt_checked_text)
                    learned = learn_charwise_with_missing(default_text, corrected_text_current)
                    if learned:
                        update_dictionary_and_untrained(learned)
                        st.success(f"è¾æ›¸ã¨å­¦ç¿’å€™è£œã« {len(learned)} ä»¶ã‚’è¿½åŠ ã—ã¾ã—ãŸï¼")
                    else:
                        st.info("ä¿®æ­£ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                    # ç”»é¢ä¿æŒã®ãŸã‚ rerun ã—ãªã„

            # TXTï¼ˆãƒšãƒ¼ã‚¸è¦‹å‡ºã—ãªã—ã§é€£çµï¼šã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å„ªå…ˆï¼‰
            final_text_page = (st.session_state.get(edit_key) or st.session_state.get(gpt_key) or gpt_checked_text or default_text).strip()
            all_corrected_texts.append(final_text_page)

            # Wordç”¨ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼šã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ç·¨é›†çµæœã‚’å„ªå…ˆ
            gpt_lines = [ln for ln in final_text_page.splitlines()]
            lines_for_layout = []
            for i, ln in enumerate(azure_lines):
                x, y = line_xy(ln)
                text_for_line = gpt_lines[i] if i < len(gpt_lines) else ln.content
                lines_for_layout.append({"text": text_for_line, "x": x, "y": y})

            page_layout_info = {
                "page_width": getattr(doc_page, "width", None) or 1.0,
                "page_height": getattr(doc_page, "height", None) or 1.0,
                "unit": getattr(doc_page, "unit", None) or "pixel",
                "lines": lines_for_layout
            }
            pages_layout.append(page_layout_info)

            done += 1; progress.progress(done / total_to_process)

        del pages, page_numbers
        gc.collect()

    status.success("âœ… ã™ã¹ã¦ã®ãƒšãƒ¼ã‚¸ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    if all_corrected_texts:
        joined_txt = "\n\n".join(all_corrected_texts)
        st.download_button(
            "ğŸ“¥ è£œæ­£ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆTXT, ãƒšãƒ¼ã‚¸è¦‹å‡ºã—ãªã—ï¼‰",
            data=joined_txt.encode("utf-8"),
            file_name="ocr_corrected.txt",
            mime="text/plain"
        )
    if pages_layout:
        try:
            docx_bytes = build_docx_from_layout(pages_layout)
            st.download_button(
                "ğŸ“¥ Wordï¼ˆ.docxï¼šãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¿‘ä¼¼ï¼‰",
                data=docx_bytes,
                file_name="ocr_layout.docx",
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
            )
        except Exception as e:
            st.warning(f"Wordå‡ºåŠ›ã«å¤±æ•—ã—ã¾ã—ãŸï¼š{e}")

# ===================== ç”»åƒï¼š1ãƒšãƒ¼ã‚¸å‡¦ç† =====================
else:
    try:
        try:
            img = Image.open(io.BytesIO(file_bytes)).convert("RGB")
        except Exception:
            arr = np.frombuffer(file_bytes, dtype=np.uint8)
            bgr = cv2.imdecode(arr, cv2.IMREAD_COLOR)
            if bgr is None:
                raise ValueError("ç”»åƒã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆJPG/PNG/PDFã®ã¿å¯¾å¿œï¼‰ã€‚")
            img = Image.fromarray(cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB))
        pages = [img]; page_numbers = [1]
    except Exception as e:
        st.exception(e); st.stop()

    all_corrected_texts: List[str] = []
    pages_layout: List[Dict[str, Any]] = []

    for page_img, page_num in zip(pages, page_numbers):
        st.write(f"## ãƒšãƒ¼ã‚¸ {page_num}")
        clean_img = remove_red_stamp(page_img)
        buf = io.BytesIO(); clean_img.save(buf, format="PNG"); buf.seek(0)

        with st.spinner("OCRã‚’å®Ÿè¡Œä¸­..."):
            t0 = time.perf_counter()
            try:
                poller = client.begin_analyze_document("prebuilt-read", document=buf)
                result = poller.result(timeout=float(ocr_timeout))
            except Exception as e:
                st.error(f"OCRãŒ{ocr_timeout}ç§’ä»¥å†…ã«å®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸ / å¤±æ•—ã—ã¾ã—ãŸï¼š{e}")
                st.caption(f"OCRå®Ÿè¡Œæ™‚é–“: {time.perf_counter() - t0:.1f}s")
                continue
            st.caption(f"OCRå®Ÿè¡Œæ™‚é–“: {time.perf_counter() - t0:.1f}s")

        doc_page = result.pages[0] if getattr(result, "pages", None) else None
        if not doc_page:
            st.warning("OCRçµæœã«ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            continue

        azure_lines = getattr(doc_page, "lines", []) or []
        default_text = "\n".join([line.content for line in azure_lines])

        dictionary = load_json_any(DICT_FILE)
        gpt_checked_text = default_text if skip_gpt else gpt_fix_text(default_text, dictionary)

        # --- ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚­ãƒ¼ ---
        ocr_key = f"ocr_{page_num}"
        gpt_key = f"gpt_{page_num}"
        edit_key = f"edit_{page_num}"

        if ocr_key not in st.session_state:
            st.session_state[ocr_key] = default_text
        if gpt_key not in st.session_state:
            st.session_state[gpt_key] = gpt_checked_text
        if edit_key not in st.session_state:
            st.session_state[edit_key] = gpt_checked_text

        tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“„ å…ƒãƒ•ã‚¡ã‚¤ãƒ«", "ğŸ–¨ï¸ OCRãƒ†ã‚­ã‚¹ãƒˆ", "ğŸ¤– GPTè£œæ­£", "âœï¸ æ‰‹ä½œæ¥­ä¿®æ­£"])
        with tab1:
            st.image(clean_img, caption=f"å…ƒãƒ•ã‚¡ã‚¤ãƒ« (ãƒšãƒ¼ã‚¸ {page_num})", use_container_width=True)
        with tab2:
            st.text_area(
                f"OCRãƒ†ã‚­ã‚¹ãƒˆï¼ˆãƒšãƒ¼ã‚¸ {page_num}ï¼‰",
                value=st.session_state.get(ocr_key, default_text),
                height=320,
                key=ocr_key
            )
        with tab3:
            st.text_area(
                f"GPTè£œæ­£ï¼ˆãƒšãƒ¼ã‚¸ {page_num}ï¼‰",
                value=st.session_state.get(gpt_key, gpt_checked_text),
                height=320,
                key=gpt_key
            )
        with tab4:
            st.text_area(
                f"æ‰‹ä½œæ¥­ä¿®æ­£ï¼ˆãƒšãƒ¼ã‚¸ {page_num}ï¼‰",
                value=st.session_state.get(edit_key, gpt_checked_text),
                height=320,
                key=edit_key
            )
            if st.button(f"ä¿®æ­£ã‚’ä¿å­˜ (ãƒšãƒ¼ã‚¸ {page_num})", key=f"save_{page_num}"):
                corrected_text_current = st.session_state.get(edit_key, gpt_checked_text)
                learned = learn_charwise_with_missing(default_text, corrected_text_current)
                if learned:
                    update_dictionary_and_untrained(learned)
                    st.success(f"è¾æ›¸ã¨å­¦ç¿’å€™è£œã« {len(learned)} ä»¶ã‚’è¿½åŠ ã—ã¾ã—ãŸï¼")
                else:
                    st.info("ä¿®æ­£ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                # rerunã—ãªã„

        # TXT
        final_text_page = (st.session_state.get(edit_key) or st.session_state.get(gpt_key) or gpt_checked_text or default_text).strip()
        all_corrected_texts.append(final_text_page)

        # Word
        gpt_lines = [ln for ln in final_text_page.splitlines()]
        lines_for_layout = []
        for i, ln in enumerate(azure_lines):
            x, y = line_xy(ln)
            text_for_line = gpt_lines[i] if i < len(gpt_lines) else ln.content
            lines_for_layout.append({"text": text_for_line, "x": x, "y": y})
        page_layout_info = {
            "page_width": getattr(doc_page, "width", None) or 1.0,
            "page_height": getattr(doc_page, "height", None) or 1.0,
            "unit": getattr(doc_page, "unit", None) or "pixel",
            "lines": lines_for_layout
        }
        pages_layout.append(page_layout_info)

    if all_corrected_texts:
        joined_txt = "\n\n".join(all_corrected_texts)
        st.download_button(
            "ğŸ“¥ è£œæ­£ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆTXT, ãƒšãƒ¼ã‚¸è¦‹å‡ºã—ãªã—ï¼‰",
            data=joined_txt.encode("utf-8"),
            file_name="ocr_corrected.txt",
            mime="text/plain"
        )
    if pages_layout:
        try:
            docx_bytes = build_docx_from_layout(pages_layout)
            st.download_button(
                "ğŸ“¥ Wordï¼ˆ.docxï¼šãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¿‘ä¼¼ï¼‰",
                data=docx_bytes,
                file_name="ocr_layout.docx",
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
            )
        except Exception as e:
            st.warning(f"Wordå‡ºåŠ›ã«å¤±æ•—ã—ã¾ã—ãŸï¼š{e}")
