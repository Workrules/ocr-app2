import streamlit as st
import pypdfium2 as pdfium
from azure.ai.formrecognizer import DocumentAnalysisClient
from azure.core.credentials import AzureKeyCredential
import os
import io
from PIL import Image
import cv2
import numpy as np
import json
import openai
import re
import difflib
from itertools import zip_longest

# ==== Azure Document Intelligence èªè¨¼æƒ…å ± ====
endpoint = os.getenv("AZURE_DOCINT_ENDPOINT")
key = os.getenv("AZURE_DOCINT_KEY")
if not endpoint or not key:
    st.error("ç’°å¢ƒå¤‰æ•° AZURE_DOCINT_ENDPOINT ã¨ AZURE_DOCINT_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()
client = DocumentAnalysisClient(endpoint=endpoint, credential=AzureKeyCredential(key))

# ==== OpenAI APIã‚­ãƒ¼ ====
openai.api_key = os.getenv("OPENAI_API_KEY")

# ==== è¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ« ====
DICT_FILE = "ocr_char_corrections.json"
UNTRAINED_FILE = "untrained_confusions.json"

JP_CHAR_RE = re.compile(r"^[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF]$")

# ==== å°å½±é™¤å» ====
def remove_red_stamp(img_pil):
    img = np.array(img_pil)
    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)
    lower_red1 = np.array([0, 70, 50])
    upper_red1 = np.array([10, 255, 255])
    lower_red2 = np.array([170, 70, 50])
    upper_red2 = np.array([180, 255, 255])
    mask1 = cv2.inRange(hsv, lower_red1, upper_red1)
    mask2 = cv2.inRange(hsv, lower_red2, upper_red2)
    mask = cv2.bitwise_or(mask1, mask2)
    img[mask > 0] = [255, 255, 255]  # ç™½å¡—ã‚Š
    return Image.fromarray(img)

# ==== JSONç®¡ç† ====
def load_json(path: str) -> dict:
    return json.load(open(path, "r", encoding="utf-8")) if os.path.exists(path) else {}

def save_json(obj: dict, path: str):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)

# ==== å­¦ç¿’ç”¨ã®èª¤èª­æŠ½å‡º ====
def learn_charwise_with_missing(original: str, corrected: str):
    learned = {}
    sm = difflib.SequenceMatcher(None, original, corrected)
    for tag, i1, i2, j1, j2 in sm.get_opcodes():
        if tag in ["replace", "insert"]:
            o_seg = original[i1:i2]
            c_seg = corrected[j1:j2]
            for o_char, c_char in zip_longest(o_seg, c_seg, fillvalue=""):
                if c_char and (not o_char or o_char != c_char):
                    wrong = o_char if o_char else "â–¡"
                    if JP_CHAR_RE.match(c_char) or c_char == "â–¡":
                        learned[wrong] = {"right": c_char, "count": 1}
    return learned

def update_dictionary_and_untrained(learned: dict):
    # OCRè£œæ­£ç”¨è¾æ›¸ã‚’æ›´æ–°
    dictionary = load_json(DICT_FILE)
    for w, meta in learned.items():
        if w in dictionary:
            if dictionary[w]["right"] == meta["right"]:
                dictionary[w]["count"] += meta["count"]
            else:
                if meta["count"] > dictionary[w]["count"]:
                    dictionary[w] = meta
        else:
            dictionary[w] = meta
    save_json(dictionary, DICT_FILE)

    # å­¦ç¿’å€™è£œãƒªã‚¹ãƒˆï¼ˆuntrainedï¼‰ã‚’æ›´æ–°
    untrained = load_json(UNTRAINED_FILE)
    for w, meta in learned.items():
        untrained[w] = meta["right"]
    save_json(untrained, UNTRAINED_FILE)

# ==== GPTè£œæ­£ ====
def gpt_fix_text(text: str, dictionary: dict) -> str:
    prompt = f"""
æ¬¡ã®OCRçµæœã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«ç›´ã—ã¦ãã ã•ã„ã€‚
- æ—¥æœ¬èªã«å­˜åœ¨ã—ãªã„æ–‡å­—ï¼ˆä¾‹: ç½—, ç½’ãªã©ï¼‰ã¯ã€Œâ–¡ã€ã«ã—ã¦ãã ã•ã„ã€‚
- è¾æ›¸å€™è£œã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„: {json.dumps(dictionary, ensure_ascii=False)}
- æ„å‘³ã‚’å‹æ‰‹ã«è£œå®Œã›ãšã€æœ€å°é™ã®ä¿®æ­£ã ã‘è¡Œã£ã¦ãã ã•ã„ã€‚

OCRçµæœ:
{text}
"""
    try:
        response = openai.chat.completions.create(
            model="gpt-5-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )
        return response.choices[0].message.content.strip()
    except Exception:
        return text

# ==== Streamlit UI ====
st.title("ğŸ“„ Document Intelligence OCR - GPTï¼‹å°å½±é™¤å»ï¼‹æ¬ è½è£œæ­£")

dictionary = load_json(DICT_FILE)
st.sidebar.subheader("ğŸ“– ç¾åœ¨ã®è¾æ›¸")
st.sidebar.json(dictionary)

uploaded_file = st.file_uploader("ç”»åƒã¾ãŸã¯PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=["jpg", "jpeg", "png", "pdf"])

if uploaded_file is not None:
    file_bytes = uploaded_file.read()
    if uploaded_file.type == "application/pdf":
        pages = convert_from_bytes(file_bytes, dpi=200)
    else:
        pages = [Image.open(io.BytesIO(file_bytes))]
    
else:
    st.info("ğŸ“‚ ã“ã“ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")



    for page_index, page_img in enumerate(pages):
        page_num = page_index + 1
        st.write(f"## ãƒšãƒ¼ã‚¸ {page_num}")

        # å°å½±é™¤å»
        clean_img = remove_red_stamp(page_img)

        # Azureã«é€ã‚‹å‰ã«PNGåœ§ç¸®
        buf = io.BytesIO()
        clean_img.save(buf, format="PNG")
        buf.seek(0)

        # OCR
        with st.spinner("OCRã‚’å®Ÿè¡Œä¸­..."):
            poller = client.begin_analyze_document("prebuilt-read", document=buf)
            result = poller.result()

        doc_page = next((p for p in result.pages if p.page_number == page_num), None)
        if not doc_page:
            st.warning("OCRçµæœã«ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            continue

        default_text = "\n".join([line.content for line in doc_page.lines])

        # GPTè£œæ­£
        gpt_checked_text = gpt_fix_text(default_text, dictionary)

        # ==== ã‚¿ãƒ– ====
        tab1, tab2, tab3, tab4 = st.tabs(
            ["ğŸ“„ å…ƒãƒ•ã‚¡ã‚¤ãƒ«", "ğŸ–¨ï¸ OCRãƒ†ã‚­ã‚¹ãƒˆ", "ğŸ¤– GPTè£œæ­£", "âœï¸ æ‰‹ä½œæ¥­ä¿®æ­£"]
        )
        with tab1:
            st.image(clean_img, caption=f"å…ƒãƒ•ã‚¡ã‚¤ãƒ« (ãƒšãƒ¼ã‚¸ {page_num})", use_container_width=True)
        with tab2:
            st.text_area(f"OCRãƒ†ã‚­ã‚¹ãƒˆï¼ˆãƒšãƒ¼ã‚¸ {page_num}ï¼‰", default_text, height=320)
        with tab3:
            st.text_area(f"GPTè£œæ­£ï¼ˆãƒšãƒ¼ã‚¸ {page_num}ï¼‰", gpt_checked_text, height=320)
        with tab4:
            corrected_text = st.text_area(
                f"æ‰‹ä½œæ¥­ä¿®æ­£ï¼ˆãƒšãƒ¼ã‚¸ {page_num}ï¼‰", gpt_checked_text, height=320, key=f"edit_{page_num}"
            )
            if st.button(f"ä¿®æ­£ã‚’ä¿å­˜ (ãƒšãƒ¼ã‚¸ {page_num})"):
                learned = learn_charwise_with_missing(default_text, corrected_text)
                if learned:
                    update_dictionary_and_untrained(learned)
                    st.success(f"è¾æ›¸ã¨å­¦ç¿’å€™è£œã« {len(learned)} ä»¶ã‚’è¿½åŠ ã—ã¾ã—ãŸï¼")
                else:
                    st.info("ä¿®æ­£ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                st.rerun()
