# dummy line
import streamlit as st
import os
import io
import json
import time
import gc
import re
import hashlib
import unicodedata
from itertools import zip_longest
from typing import List, Tuple, Dict, Any

import numpy as np
import cv2
from PIL import Image, ImageFile
import pypdfium2 as pdfium

from azure.ai.formrecognizer import DocumentAnalysisClient
from azure.core.credentials import AzureKeyCredential

from openai import OpenAI

from docx import Document
from docx.shared import Cm, Pt

# ===================== åŸºæœ¬è¨­å®š =====================
AZURE_DOCINT_ENDPOINT = os.getenv("AZURE_DOCINT_ENDPOINT") or st.secrets.get("AZURE_DOCINT_ENDPOINT")
AZURE_DOCINT_KEY = os.getenv("AZURE_DOCINT_KEY") or st.secrets.get("AZURE_DOCINT_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY")
MODEL_NAME = os.getenv("OCR_GPT_MODEL") or st.secrets.get("OCR_GPT_MODEL", "gpt-5")
BATCH_SIZE_DEFAULT = max(1, int(os.getenv("OCR_BATCH_PAGES") or st.secrets.get("OCR_BATCH_PAGES", "10")))

if not AZURE_DOCINT_ENDPOINT or not AZURE_DOCINT_KEY:
    st.error("ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯Secretsã§ AZURE_DOCINT_ENDPOINT / AZURE_DOCINT_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()

client = DocumentAnalysisClient(endpoint=AZURE_DOCINT_ENDPOINT, credential=AzureKeyCredential(AZURE_DOCINT_KEY))
openai_client = OpenAI(api_key=OPENAI_API_KEY)

JP_CHAR_RE = re.compile(r"^[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF]$")

# ===================== ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸é¸æŠï¼ˆlocal / azureblobï¼‰ =====================
STORAGE_BACKEND = os.getenv("OCR_DICT_BACKEND") or st.secrets.get("OCR_DICT_BACKEND", "local")

if STORAGE_BACKEND == "azureblob":
    try:
        from azure.storage.blob import BlobServiceClient, ContentSettings
        from azure.core.exceptions import ResourceNotFoundError, ResourceExistsError
    except Exception as e:
        st.error(f"azure-storage-blob ãŒå¿…è¦ã§ã™ã€‚requirements.txt ã«è¿½åŠ ã—ã¦ãã ã•ã„ã€‚è©³ç´°: {e}")
        st.stop()

    AZURE_BLOB_CONN_STR = os.getenv("AZURE_BLOB_CONN_STR") or st.secrets.get("AZURE_BLOB_CONN_STR")
    OCR_DICT_CONTAINER = os.getenv("OCR_DICT_CONTAINER") or st.secrets.get("OCR_DICT_CONTAINER", "ocr-shared-dict")
    if not AZURE_BLOB_CONN_STR:
        st.error("Secrets/ç’°å¢ƒå¤‰æ•° AZURE_BLOB_CONN_STR ã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼ˆBlobæ¥ç¶šæ–‡å­—åˆ—ï¼‰ã€‚")
        st.stop()

    _blob_service = BlobServiceClient.from_connection_string(AZURE_BLOB_CONN_STR)
    _container = _blob_service.get_container_client(OCR_DICT_CONTAINER)
    try:
        _container.create_container()
    except ResourceExistsError:
        pass

    DICT_FILE = "ocr_char_corrections.json"
    UNTRAINED_FILE = "untrained_confusions.json"
    TRAINED_FILE = "trained_confusions.json"

    def _load_json_blob(blob_name: str) -> dict:
        try:
            data = _container.download_blob(blob_name).readall()
            return json.loads(data.decode("utf-8"))
        except Exception:
            return {}

    def _save_json_blob(obj: dict, blob_name: str):
        b = json.dumps(obj, ensure_ascii=False, indent=2).encode("utf-8")
        _container.upload_blob(
            name=blob_name, data=b, overwrite=True,
            content_settings=ContentSettings(content_type="application/json; charset=utf-8"),
        )

    def load_json_any(key: str) -> dict:
        return _load_json_blob(key)
    def save_json_any(obj: dict, key: str):
        _save_json_blob(obj, key)

elif STORAGE_BACKEND == "local":
    DICT_DIR = os.getenv("OCR_DICT_DIR") or st.secrets.get("OCR_DICT_DIR", ".")
    DICT_FILE = os.path.join(DICT_DIR, "ocr_char_corrections.json")
    UNTRAINED_FILE = os.path.join(DICT_DIR, "untrained_confusions.json")
    TRAINED_FILE = os.path.join(DICT_DIR, "trained_confusions.json")

    def _load_json_local(path: str, retries: int = 3, delay: float = 0.1) -> dict:
        for _ in range(retries):
            try:
                with open(path, "r", encoding="utf-8") as f:
                    return json.load(f)
            except FileNotFoundError:
                return {}
            except (json.JSONDecodeError, PermissionError):
                time.sleep(delay)
        return {}

    def _save_json_local(obj: dict, path: str):
        os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
        tmp_path = f"{path}.tmp"
        with open(tmp_path, "w", encoding="utf-8") as f:
            json.dump(obj, f, ensure_ascii=False, indent=2)
            f.flush(); os.fsync(f.fileno())
        os.replace(tmp_path, path)

    def load_json_any(key: str) -> dict:
        return _load_json_local(key)
    def save_json_any(obj: dict, key: str):
        _save_json_local(obj, key)
else:
    st.error(f"æœªçŸ¥ã® OCR_DICT_BACKEND: {STORAGE_BACKEND}ï¼ˆlocal / azureblob ã®ã¿å¯¾å¿œï¼‰")
    st.stop()

# ===================== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =====================
def remove_red_stamp(img_pil: Image.Image) -> Image.Image:
    img = np.array(img_pil)
    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)
    lower_red1 = np.array([0, 70, 50]); upper_red1 = np.array([10, 255, 255])
    lower_red2 = np.array([170, 70, 50]); upper_red2 = np.array([180, 255, 255])
    mask = cv2.bitwise_or(cv2.inRange(hsv, lower_red1, upper_red1), cv2.inRange(hsv, lower_red2, upper_red2))
    img[mask > 0] = [255, 255, 255]
    return Image.fromarray(img)

def learn_charwise_with_missing(original: str, corrected: str) -> dict:
    learned: Dict[str, Dict[str, Any]] = {}
    import difflib
    sm = difflib.SequenceMatcher(None, original, corrected)
    for tag, i1, i2, j1, j2 in sm.get_opcodes():
        if tag in ["replace", "insert"]:
            o_seg, c_seg = original[i1:i2], corrected[j1:j2]
            for o_char, c_char in zip_longest(o_seg, c_seg, fillvalue=""):
                if c_char and (not o_char or o_char != c_char):
                    wrong = o_char if o_char else "â–¡"
                    if JP_CHAR_RE.match(c_char) or c_char == "â–¡":
                        learned[wrong] = {"right": c_char, "count": 1}
    return learned

def update_dictionary_and_untrained(learned: dict):
    dictionary = load_json_any(DICT_FILE)
    for w, meta in learned.items():
        if w in dictionary:
            if dictionary[w]["right"] == meta["right"]:
                dictionary[w]["count"] += meta["count"]
            else:
                if meta["count"] > dictionary[w]["count"]:
                    dictionary[w] = meta
        else:
            dictionary[w] = meta
    save_json_any(dictionary, DICT_FILE)
    untrained = load_json_any(UNTRAINED_FILE)
    for w, meta in learned.items():
        untrained[w] = meta["right"]
    save_json_any(untrained, UNTRAINED_FILE)

def gpt_fix_text(text: str, dictionary: dict) -> str:
    prompt = f"""
æ¬¡ã®OCRçµæœã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«ç›´ã—ã¦ãã ã•ã„ã€‚
- æ—¥æœ¬èªã«å­˜åœ¨ã—ãªã„æ–‡å­—ã¯ã€Œâ–¡ã€ã«ã—ã¦ãã ã•ã„ã€‚
- è¾æ›¸å€™è£œã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„: {json.dumps(dictionary, ensure_ascii=False)}
- æ„å‘³ã‚’å‹æ‰‹ã«è£œå®Œã›ãšã€æœ€å°é™ã®ä¿®æ­£ã ã‘è¡Œã£ã¦ãã ã•ã„ã€‚

OCRçµæœ:
{text}
""".strip()
    try:
        resp = openai_client.responses.create(
            model=MODEL_NAME,
            input=prompt,
            text={"verbosity": "low"},
            reasoning={"effort": "minimal"},
        )
        if hasattr(resp, "output_text") and resp.output_text:
            return resp.output_text.strip()
        out_parts = []
        for item in getattr(resp, "output", []) or []:
            for part in getattr(item, "content", []) or []:
                t = getattr(part, "text", None)
                if t: out_parts.append(t)
        out = "".join(out_parts).strip()
        return out or text
    except Exception as e:
        st.warning(f"GPTè£œæ­£ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰ï¼š{e}")
        return text

# ========== PDFãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚° ==========
def is_pdf(b: bytes) -> bool:
    return len(b) >= 5 and b[:5] == b"%PDF-"

def render_pdf_selected_pages(pdf_bytes: bytes, indices_0based: List[int], dpi: int = 200) -> Tuple[List[Image.Image], List[int]]:
    imgs: List[Image.Image] = []
    nums: List[int] = []
    pdf = pdfium.PdfDocument(io.BytesIO(pdf_bytes))
    scale = dpi / 72.0
    for idx in indices_0based:
        page = pdf[idx]
        pil = page.render(scale=scale).to_pil().convert("RGB")
        imgs.append(pil); nums.append(idx + 1)
    return imgs, nums

# ====== â˜… ãƒšãƒ¼ã‚¸æŒ‡å®šã®æ­£è¦åŒ–ãƒ»è§£æï¼ˆå…¨è§’å¯¾å¿œï¼‰ ======
def parse_page_spec(spec: str, max_pages: int) -> List[int]:
    """
    '1,3,5-7' ãªã©ã‚’ 0å§‹ã¾ã‚Šã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¸ã€‚
    å…¨è§’æ•°å­—/ã‚«ãƒ³ãƒ/ãƒã‚¤ãƒ•ãƒ³/é•·éŸ³ã‚‚è¨±å¯ï¼ˆä¾‹ï¼š'ï¼‘ï¼Œï¼“ï¼Œï¼•ï¼ï¼—' '1ãƒ¼3' '1â€”3' '1â€“3' '1â€•3'ï¼‰
    """
    s = (spec or "").strip()
    if not s:
        return []
    # å…¨è§’â†’åŠè§’ã«æ­£è¦åŒ–
    s = unicodedata.normalize("NFKC", s)
    # æ—¥æœ¬èªã‚«ãƒ³ãƒãªã©ã‚’åŠè§’ã‚«ãƒ³ãƒã«ã€å„ç¨®ãƒ€ãƒƒã‚·ãƒ¥ã‚’åŠè§’ãƒã‚¤ãƒ•ãƒ³ã«
    s = s.replace("ï¼Œ", ",").replace("ã€", ",")
    for dash in ["ï¼", "ãƒ¼", "â€•", "â€”", "â€“"]:
        s = s.replace(dash, "-")
    # åˆ†å‰²
    out = set()
    parts = [p.strip() for p in s.split(",") if p.strip()]
    for p in parts:
        if "-" in p:
            a, b = p.split("-", 1)
            try:
                ia = int(a); ib = int(b)
                lo, hi = min(ia, ib), max(ia, ib)
                lo = max(1, lo); hi = min(max_pages, hi)
                for n in range(lo, hi + 1):
                    out.add(n - 1)
            except ValueError:
                # ç„¡åŠ¹ãƒˆãƒ¼ã‚¯ãƒ³ã¯ç„¡è¦–
                continue
        else:
            try:
                n = int(p)
                if 1 <= n <= max_pages:
                    out.add(n - 1)
            except ValueError:
                continue
    return sorted(out)

def chunked(seq: List[int], n: int) -> List[List[int]]:
    return [seq[i:i+n] for i in range(0, len(seq), n)]

# ========== Azureè¡Œã®åº§æ¨™ ==========
def line_xy(line_obj: Any) -> Tuple[float, float]:
    poly = getattr(line_obj, "polygon", None) or getattr(line_obj, "bounding_polygon", None)
    if not poly: return (0.0, 0.0)
    xs, ys = [], []
    for p in poly:
        x = getattr(p, "x", None); y = getattr(p, "y", None)
        if x is None and isinstance(p, dict):
            x = p.get("x", 0.0); y = p.get("y", 0.0)
        xs.append(float(x)); ys.append(float(y))
    return (min(xs or [0.0]), min(ys or [0.0]))

# ========== Wordç”Ÿæˆ ==========
EMU_PER_CM = 360000.0
def to_cm(val) -> float:
    try:
        return float(getattr(val, "cm"))
    except Exception:
        try:
            return float(val) / EMU_PER_CM
        except Exception:
            return float(val)

def build_docx_from_layout(pages_layout: List[Dict[str, Any]]) -> bytes:
    doc = Document()
    section = doc.sections[0]
    section.page_width = Cm(21.0); section.page_height = Cm(29.7)
    section.left_margin = Cm(2.0); section.right_margin = Cm(2.0)
    section.top_margin = Cm(2.0); section.bottom_margin = Cm(2.0)

    page_w_cm = to_cm(section.page_width)
    left_cm = to_cm(section.left_margin); right_cm = to_cm(section.right_margin)
    content_width_cm = max(0.1, page_w_cm - left_cm - right_cm)

    style = doc.styles["Normal"]; style.font.name = "Yu Gothic"; style.font.size = Pt(11)

    for idx, page in enumerate(pages_layout, start=1):
        pw = float(page.get("page_width") or 1.0)
        ph = float(page.get("page_height") or 1.0)
        lines = page.get("lines", [])
        lines_sorted = sorted(lines, key=lambda r: (r["y"], r["x"]))
        y_thresh = ph * 0.018
        prev_y = None

        for item in lines_sorted:
            txt = item["text"]; x = float(item["x"]); y = float(item["y"])
            para = doc.add_paragraph(); para.add_run(txt)
            indent_cm = max(0.0, min(0.9 * content_width_cm, (x / max(pw, 1e-6)) * content_width_cm))
            para.paragraph_format.left_indent = Cm(indent_cm)
            para.paragraph_format.space_after = Pt(2)
            if prev_y is not None and (y - prev_y) > y_thresh:
                para.paragraph_format.space_before = Pt(8)
            prev_y = y

        if idx < len(pages_layout):
            doc.add_page_break()

    bio = io.BytesIO(); doc.save(bio); bio.seek(0)
    return bio.read()

# ===================== UI =====================
st.title("ğŸ“„ Document Intelligence OCRï¼ˆAzureï¼‰â€” ãƒšãƒ¼ã‚¸æŒ‡å®š/ãƒãƒƒãƒ/GPT/Word/çŠ¶æ…‹ä¿æŒ/ãƒ­ã‚°å¼·åŒ–")

# è¨ºæ–­
st.sidebar.markdown("### ğŸ”§ ç’°å¢ƒ")
st.sidebar.write({
    "AZURE_DOCINT_ENDPOINT_set": bool(AZURE_DOCINT_ENDPOINT),
    "AZURE_DOCINT_KEY_set": bool(AZURE_DOCINT_KEY),
    "OPENAI_API_KEY_set": bool(OPENAI_API_KEY),
    "MODEL": MODEL_NAME,
    "BATCH_SIZE(default)": BATCH_SIZE_DEFAULT,
})

st.sidebar.markdown("### ğŸ“‚ è¾æ›¸ã®å‚ç…§å…ˆ")
if STORAGE_BACKEND == "azureblob":
    st.sidebar.write({
        "OCR_DICT_BACKEND": "azureblob",
        "container": os.getenv("OCR_DICT_CONTAINER") or st.secrets.get("OCR_DICT_CONTAINER", "ocr-shared-dict"),
        "DICT_BLOB": DICT_FILE, "UNTRAINED_BLOB": UNTRAINED_FILE, "TRAINED_BLOB": TRAINED_FILE,
    })
else:
    st.sidebar.write({
        "OCR_DICT_BACKEND": "local",
        "OCR_DICT_DIR": os.path.abspath(os.getenv("OCR_DICT_DIR") or st.secrets.get("OCR_DICT_DIR", ".")),
        "DICT_FILE": os.path.abspath(DICT_FILE),
        "UNTRAINED_FILE": os.path.abspath(UNTRAINED_FILE),
        "TRAINED_FILE": os.path.abspath(TRAINED_FILE),
    })

# ãƒ‡ãƒãƒƒã‚°UI
st.sidebar.markdown("### ğŸ›  ãƒ‡ãƒãƒƒã‚°")
skip_gpt = st.sidebar.checkbox("GPTè£œæ­£ã‚’ã‚¹ã‚­ãƒƒãƒ—", value=False)
ocr_timeout = st.sidebar.slider("OCRã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰", 10, 120, 45, step=5)
batch_size_override = st.sidebar.number_input("ãƒãƒƒãƒã‚µã‚¤ã‚ºä¸Šæ›¸ã", 1, 20, value=BATCH_SIZE_DEFAULT)
use_cache = st.sidebar.checkbox("OCRã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆå®Ÿé¨“ï¼‰", value=False)
debug_log = st.sidebar.checkbox("ğŸ” è©³ç´°ãƒ­ã‚°", value=True)

# è¾æ›¸ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆæ‰‹å‹•å†èª­è¾¼ï¼‰
dict_preview_box = st.sidebar.container()
with dict_preview_box:
    st.subheader("ğŸ“– ç¾åœ¨ã®è¾æ›¸ï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰")
    st.json(load_json_any(DICT_FILE))
if st.sidebar.button("ğŸ”„ è¾æ›¸ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å†èª­è¾¼", type="secondary"):
    with dict_preview_box:
        st.subheader("ğŸ“– ç¾åœ¨ã®è¾æ›¸ï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰")
        st.json(load_json_any(DICT_FILE))

# ===================== ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¿æŒ =====================
if "file_bytes" not in st.session_state:
    st.session_state["file_bytes"] = None
    st.session_state["file_name"] = None
    st.session_state["file_mime"] = None
    st.session_state["is_pdf"] = False
    st.session_state["dpi"] = 200
    st.session_state["page_indices"] = []
    st.session_state["ran"] = False

uploaded = st.file_uploader("ç”»åƒã¾ãŸã¯PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=["jpg", "jpeg", "png", "pdf"], key="uploader")
if uploaded is not None:
    st.session_state["file_bytes"] = uploaded.getvalue()
    st.session_state["file_name"] = uploaded.name
    st.session_state["file_mime"] = uploaded.type
    st.session_state["is_pdf"] = (uploaded.type == "application/pdf") or uploaded.name.lower().endswith(".pdf")
    # æ—¢å­˜çŠ¶æ…‹ã®åˆæœŸåŒ–
    for k in list(st.session_state.keys()):
        if k.startswith("ocr_") or k.startswith("gpt_") or k.startswith("edit_"):
            del st.session_state[k]
    st.session_state["page_indices"] = []
    st.session_state["ran"] = False

file_bytes = st.session_state["file_bytes"]
if not file_bytes:
    st.info("ğŸ“‚ ã“ã“ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
    st.stop()

ImageFile.LOAD_TRUNCATED_IMAGES = True
is_input_pdf = st.session_state["is_pdf"]

# ====== ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šå®Ÿè¡Œã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ï¼ˆå¸¸æ™‚è¡¨ç¤ºã§â€œå‹•ã„ã¦ãªã„â€ã‚’å¯è¦–åŒ–ï¼‰ ======
st.sidebar.markdown("### ğŸ“Š å®Ÿè¡Œã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹")
st.sidebar.write({
    "ran": bool(st.session_state.get("ran")),
    "has_file": bool(file_bytes),
    "is_pdf": bool(is_input_pdf),
    "saved_indices": st.session_state.get("page_indices", []),
    "dpi": st.session_state.get("dpi", 200),
})

# ===================== OCRã‚³ã‚¢ï¼ˆæ‰‹å‹•ãƒãƒ¼ãƒªãƒ³ã‚°ï¼‰ =====================
def _ocr_polling(png_bytes: bytes, timeout_sec: float) -> dict:
    poller = client.begin_analyze_document("prebuilt-read", document=io.BytesIO(png_bytes))
    t0 = time.perf_counter()
    status = st.empty()
    while True:
        if poller.done():
            break
        elapsed = time.perf_counter() - t0
        if elapsed > float(timeout_sec):
            status.empty()
            raise TimeoutError(f"Azure OCR timeout after {elapsed:.1f}s")
        status.info(f"Azure OCR å®Ÿè¡Œä¸­â€¦ {elapsed:.1f}s / {timeout_sec:.0f}s")
        time.sleep(0.3)
    status.empty()

    result = poller.result()
    doc_page = result.pages[0] if getattr(result, "pages", None) else None
    if not doc_page:
        return {"pw": 1.0, "ph": 1.0, "lines": [], "raw": getattr(result, "content", "")}

    lines = []
    for ln in getattr(doc_page, "lines", []) or []:
        x, y = line_xy(ln)
        lines.append({"content": ln.content, "x": float(x), "y": float(y)})

    return {
        "pw": float(getattr(doc_page, "width", 1.0) or 1.0),
        "ph": float(getattr(doc_page, "height", 1.0) or 1.0),
        "lines": lines,
        "raw": getattr(result, "content", "") or "\n".join([l["content"] for l in lines]),
    }

@st.cache_data(show_spinner=False)
def _ocr_cached(digest: str, png_bytes: bytes, timeout_sec: float) -> dict:
    return _ocr_polling(png_bytes, timeout_sec)

def _ocr_dispatch(png_bytes: bytes, timeout_sec: float, use_cache: bool) -> dict:
    if use_cache:
        digest = hashlib.md5(png_bytes).hexdigest()
        return _ocr_cached(digest, png_bytes, timeout_sec)
    else:
        return _ocr_polling(png_bytes, timeout_sec)

# ===================== ãƒšãƒ¼ã‚¸é¸æŠï¼ˆå¸¸æ™‚UIï¼‹å®Ÿè¡Œãƒœã‚¿ãƒ³ï¼‰ =====================
if is_input_pdf:
    # PDFãƒšãƒ¼ã‚¸æ•°
    try:
        pdf_for_count = pdfium.PdfDocument(io.BytesIO(file_bytes))
        total_pages = len(pdf_for_count)
        st.info(f"ğŸ“˜ ã“ã®PDFã¯ {total_pages} ãƒšãƒ¼ã‚¸ã‚ã‚Šã¾ã™ã€‚")
    except Exception as e:
        st.exception(e); st.stop()

    st.subheader("â–¶ OCRã™ã‚‹ãƒšãƒ¼ã‚¸ã‚’é¸æŠ")
    col1, col2 = st.columns([2,1])
    with col1:
        select_mode = st.radio(
            "é¸æŠæ–¹æ³•",
            options=["å…¨ãƒšãƒ¼ã‚¸", "ç¯„å›²æŒ‡å®š", "ãƒšãƒ¼ã‚¸ç•ªå·æŒ‡å®šï¼ˆä¾‹: 1,3,5-7 / ï¼‘ï¼Œï¼“ï¼Œï¼•ï¼ï¼—ï¼‰"],
            index=1 if total_pages > 1 else 0,
            horizontal=True
        )
    with col2:
        dpi = st.slider("DPI", 72, 300, value=st.session_state.get("dpi", 200), step=4)
        st.session_state["dpi"] = dpi

    if select_mode == "ç¯„å›²æŒ‡å®š" and total_pages > 1:
        start, end = st.slider("å‡¦ç†ã™ã‚‹ãƒšãƒ¼ã‚¸ç¯„å›²ï¼ˆ1å§‹ã¾ã‚Šï¼‰", 1, total_pages, (1, min(total_pages, 5)))
        chosen_indices = list(range(start - 1, end))
    elif select_mode == "ãƒšãƒ¼ã‚¸ç•ªå·æŒ‡å®šï¼ˆä¾‹: 1,3,5-7 / ï¼‘ï¼Œï¼“ï¼Œï¼•ï¼ï¼—ï¼‰":
        spec_default = "1-3" if total_pages >= 3 else "1"
        spec = st.text_input("ãƒšãƒ¼ã‚¸ç•ªå·ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã€ç¯„å›²ã¯ãƒã‚¤ãƒ•ãƒ³ã€‚å…¨è§’OKï¼‰", value=spec_default)
        chosen_indices = parse_page_spec(spec, total_pages)
    else:
        chosen_indices = list(range(total_pages))

    st.caption(f"é¸æŠä¸­: {', '.join(str(i+1) for i in chosen_indices) if chosen_indices else '(ãªã—)'} / DPI={dpi}")

    run_clicked = st.button("â–¶ ã“ã®è¨­å®šã§OCRã‚’å®Ÿè¡Œ", type="primary")
   # 1) ã‚¯ãƒªãƒƒã‚¯ã•ã‚ŒãŸã‹ã€éå»ã«å®Ÿè¡Œæ¸ˆã¿ï¼ˆran=Trueï¼‰ãªã‚‰èµ°ã‚‹
# 2) ã•ã‚‰ã«ã€page_indices ãŒç©ºãªã‚‰ä»Šå›ã® chosen_indices ã‚’æ¡ç”¨
if (run_clicked or st.session_state.get("ran")) and chosen_indices:
    if not st.session_state.get("page_indices"):
        st.session_state["page_indices"] = chosen_indices
    st.session_state["ran"] = True
else:
    # é¸æŠãŒç©º or ã¾ã å®Ÿè¡Œãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚Œã¦ã„ãªã„
    if not chosen_indices:
        st.error("é¸æŠã•ã‚ŒãŸãƒšãƒ¼ã‚¸ãŒç©ºã§ã™ã€‚ã€å…¨ãƒšãƒ¼ã‚¸ã€ã«åˆ‡ã‚Šæ›¿ãˆã‚‹ã‹ã€ç¯„å›²/ãƒšãƒ¼ã‚¸ç•ªå·ã‚’å…¥ã‚Œã¦ãã ã•ã„ã€‚")
    else:
        st.warning("å®Ÿè¡Œå¾…ã¡ã§ã™ã€‚ã€â–¶ ã“ã®è¨­å®šã§OCRã‚’å®Ÿè¡Œã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

    dpi = st.session_state.get("dpi", 200)
    EFFECTIVE_BATCH = int(batch_size_override) if batch_size_override else BATCH_SIZE_DEFAULT
    total_to_process = len(chosen_indices)
    progress = st.progress(0.0)
    status_area = st.empty()
    all_corrected_texts: List[str] = []
    pages_layout: List[Dict[str, Any]] = []
    done = 0

    st.write("### â–¶ å®Ÿè¡Œé–‹å§‹")
    st.write(f"ğŸ§ª ãƒšãƒ¼ã‚¸: {', '.join(str(i+1) for i in chosen_indices)} / DPI={dpi} / ãƒãƒƒãƒ={EFFECTIVE_BATCH}")

    for batch_no, batch_indices in enumerate(chunked(chosen_indices, EFFECTIVE_BATCH), start=1):
        status_area.info(f"ğŸ”„ ãƒãƒƒãƒ {batch_no} / {((total_to_process - 1) // EFFECTIVE_BATCH) + 1} ï¼ˆãƒšãƒ¼ã‚¸: {', '.join(str(i+1) for i in batch_indices)}ï¼‰")
        try:
            st.write(f"ğŸ–¼ ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°é–‹å§‹ï¼ˆ{len(batch_indices)}ãƒšãƒ¼ã‚¸ï¼‰...")
            pages, page_numbers = render_pdf_selected_pages(file_bytes, batch_indices, dpi=dpi)
            st.write(f"âœ… ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å®Œäº†: {len(pages)}ãƒšãƒ¼ã‚¸")
        except Exception as e:
            st.exception(e); st.stop()

        for page_img, page_num in zip(pages, page_numbers):
            st.write(f"## ãƒšãƒ¼ã‚¸ {page_num}")
            clean_img = remove_red_stamp(page_img)
            st.image(clean_img, caption=f"å…ƒãƒ•ã‚¡ã‚¤ãƒ« (ãƒšãƒ¼ã‚¸ {page_num})", use_container_width=True)

            buf = io.BytesIO(); clean_img.save(buf, format="PNG"); png_bytes = buf.getvalue()

            with st.spinner("OCRã‚’å®Ÿè¡Œä¸­..."):
                t0 = time.perf_counter()
                try:
                    cached = _ocr_dispatch(png_bytes, ocr_timeout, use_cache)
                except Exception as e:
                    st.error(f"OCRã«å¤±æ•—ï¼š{e}")
                    st.caption(f"OCRå®Ÿè¡Œæ™‚é–“: {time.perf_counter() - t0:.1f}s")
                    continue
                elapsed = time.perf_counter() - t0
                st.caption(f"OCRå®Ÿè¡Œæ™‚é–“: {elapsed:.1f}s")

            azure_lines = cached.get("lines") or []
            default_text = "\n".join([ln["content"] for ln in azure_lines]) if azure_lines else (cached.get("raw") or "")
            if not default_text.strip():
                st.warning("OCRã¯æˆåŠŸã—ã¾ã—ãŸãŒãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã—ãŸã€‚DPIã‚„ç”»åƒå“è³ªã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")

            dictionary = load_json_any(DICT_FILE)
            gpt_checked_text = default_text if skip_gpt else gpt_fix_text(default_text, dictionary)

            ocr_key = f"ocr_{page_num}"
            gpt_key = f"gpt_{page_num}"
            edit_key = f"edit_{page_num}"
            if ocr_key not in st.session_state: st.session_state[ocr_key] = default_text
            if gpt_key not in st.session_state: st.session_state[gpt_key] = gpt_checked_text
            if edit_key not in st.session_state: st.session_state[edit_key] = gpt_checked_text

            tab2, tab3, tab4 = st.tabs(["ğŸ–¨ï¸ OCRãƒ†ã‚­ã‚¹ãƒˆ", "ğŸ¤– GPTè£œæ­£", "âœï¸ æ‰‹ä½œæ¥­ä¿®æ­£"])
            with tab2:
                st.text_area(f"OCRãƒ†ã‚­ã‚¹ãƒˆï¼ˆãƒšãƒ¼ã‚¸ {page_num}ï¼‰", height=320, key=ocr_key)
            with tab3:
                st.text_area(f"GPTè£œæ­£ï¼ˆãƒšãƒ¼ã‚¸ {page_num}ï¼‰", height=320, key=gpt_key)
            with tab4:
                st.text_area(f"æ‰‹ä½œæ¥­ä¿®æ­£ï¼ˆãƒšãƒ¼ã‚¸ {page_num}ï¼‰", height=320, key=edit_key)
                if st.button(f"ä¿®æ­£ã‚’ä¿å­˜ (ãƒšãƒ¼ã‚¸ {page_num})", key=f"save_{page_num}"):
                    corrected_text_current = st.session_state.get(edit_key, gpt_checked_text)
                    learned = learn_charwise_with_missing(st.session_state.get(ocr_key, default_text), corrected_text_current)
                    if learned:
                        update_dictionary_and_untrained(learned)
                        st.success(f"è¾æ›¸ã¨å­¦ç¿’å€™è£œã« {len(learned)} ä»¶ã‚’è¿½åŠ ã—ã¾ã—ãŸï¼")
                    else:
                        st.info("ä¿®æ­£ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")

            final_text_page = (
                st.session_state.get(edit_key)
                or st.session_state.get(gpt_key)
                or gpt_checked_text
                or default_text
            ).strip()
            all_corrected_texts.append(final_text_page)

            gpt_lines = final_text_page.splitlines()
            lines_for_layout = []
            for i, ln in enumerate(azure_lines):
                x, y = ln["x"], ln["y"]
                text_for_line = gpt_lines[i] if i < len(gpt_lines) else ln["content"]
                lines_for_layout.append({"text": text_for_line, "x": x, "y": y})
            pages_layout.append({
                "page_width": cached["pw"], "page_height": cached["ph"], "unit": "pixel",
                "lines": lines_for_layout
            })

            done += 1; progress.progress(done / total_to_process)

        del pages, page_numbers
        gc.collect()

    status_area.success("âœ… ã™ã¹ã¦ã®ãƒšãƒ¼ã‚¸ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    if all_corrected_texts:
        joined_txt = "\n\n".join(all_corrected_texts)
        st.download_button(
            "ğŸ“¥ è£œæ­£ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆTXT, ãƒšãƒ¼ã‚¸è¦‹å‡ºã—ãªã—ï¼‰",
            data=joined_txt.encode("utf-8"), file_name="ocr_corrected.txt", mime="text/plain"
        )
    if pages_layout:
        try:
            st.download_button(
                "ğŸ“¥ Wordï¼ˆ.docxï¼šãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¿‘ä¼¼ï¼‰",
                data=build_docx_from_layout(pages_layout),
                file_name="ocr_layout.docx",
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            )
        except Exception as e:
            st.warning(f"Wordå‡ºåŠ›ã«å¤±æ•—ã—ã¾ã—ãŸï¼š{e}")

# ===================== ç”»åƒï¼š1ãƒšãƒ¼ã‚¸å‡¦ç† =====================
else:
    try:
        try:
            img = Image.open(io.BytesIO(file_bytes)).convert("RGB")
        except Exception:
            arr = np.frombuffer(file_bytes, dtype=np.uint8)
            bgr = cv2.imdecode(arr, cv2.IMREAD_COLOR)
            if bgr is None:
                raise ValueError("ç”»åƒã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆJPG/PNG/PDFã®ã¿å¯¾å¿œï¼‰ã€‚")
            img = Image.fromarray(cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB))
        pages = [img]; page_numbers = [1]
    except Exception as e:
        st.exception(e); st.stop()

    st.caption("å˜ä¸€ç”»åƒã¨ã—ã¦OCRã—ã¾ã™ã€‚")
    run_img = st.button("â–¶ ã“ã®ç”»åƒã§OCRã‚’å®Ÿè¡Œ", type="primary")
    if not run_img and not st.session_state.get("ran"):
        st.stop()
    st.session_state["ran"] = True

    all_corrected_texts: List[str] = []
    pages_layout: List[Dict[str, Any]] = []

    for page_img, page_num in zip(pages, page_numbers):
        st.write(f"## ãƒšãƒ¼ã‚¸ {page_num}")
        clean_img = remove_red_stamp(page_img)
        st.image(clean_img, caption=f"å…ƒãƒ•ã‚¡ã‚¤ãƒ« (ãƒšãƒ¼ã‚¸ {page_num})", use_container_width=True)

        buf = io.BytesIO(); clean_img.save(buf, format="PNG"); png_bytes = buf.getvalue()

        with st.spinner("OCRã‚’å®Ÿè¡Œä¸­..."):
            t0 = time.perf_counter()
            try:
                cached = _ocr_dispatch(png_bytes, ocr_timeout, use_cache)
            except Exception as e:
                st.error(f"OCRã«å¤±æ•—ï¼š{e}")
                st.caption(f"OCRå®Ÿè¡Œæ™‚é–“: {time.perf_counter() - t0:.1f}s")
                continue
            elapsed = time.perf_counter() - t0
            st.caption(f"OCRå®Ÿè¡Œæ™‚é–“: {elapsed:.1f}s")

        azure_lines = cached.get("lines") or []
        default_text = "\n".join([ln["content"] for ln in azure_lines]) if azure_lines else (cached.get("raw") or "")
        if not default_text.strip():
            st.warning("OCRã¯æˆåŠŸã—ã¾ã—ãŸãŒãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã—ãŸã€‚DPIã‚„ç”»åƒå“è³ªã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")

        dictionary = load_json_any(DICT_FILE)
        gpt_checked_text = default_text if skip_gpt else gpt_fix_text(default_text, dictionary)

        ocr_key = f"ocr_{page_num}"; gpt_key = f"gpt_{page_num}"; edit_key = f"edit_{page_num}"
        if ocr_key not in st.session_state: st.session_state[ocr_key] = default_text
        if gpt_key not in st.session_state: st.session_state[gpt_key] = gpt_checked_text
        if edit_key not in st.session_state: st.session_state[edit_key] = gpt_checked_text

        tab2, tab3, tab4 = st.tabs(["ğŸ–¨ï¸ OCRãƒ†ã‚­ã‚¹ãƒˆ", "ğŸ¤– GPTè£œæ­£", "âœï¸ æ‰‹ä½œæ¥­ä¿®æ­£"])
        with tab2: st.text_area(f"OCRãƒ†ã‚­ã‚¹ãƒˆï¼ˆãƒšãƒ¼ã‚¸ {page_num}ï¼‰", height=320, key=ocr_key)
        with tab3: st.text_area(f"GPTè£œæ­£ï¼ˆãƒšãƒ¼ã‚¸ {page_num}ï¼‰", height=320, key=gpt_key)
        with tab4:
            st.text_area(f"æ‰‹ä½œæ¥­ä¿®æ­£ï¼ˆãƒšãƒ¼ã‚¸ {page_num}ï¼‰", height=320, key=edit_key)
            if st.button(f"ä¿®æ­£ã‚’ä¿å­˜ (ãƒšãƒ¼ã‚¸ {page_num})", key=f"save_{page_num}"):
                corrected_text_current = st.session_state.get(edit_key, gpt_checked_text)
                learned = learn_charwise_with_missing(st.session_state.get(ocr_key, default_text), corrected_text_current)
                if learned:
                    update_dictionary_and_untrained(learned)
                    st.success(f"è¾æ›¸ã¨å­¦ç¿’å€™è£œã« {len(learned)} ä»¶ã‚’è¿½åŠ ã—ã¾ã—ãŸï¼")
                else:
                    st.info("ä¿®æ­£ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")

        final_text_page = (st.session_state.get(edit_key) or st.session_state.get(gpt_key) or gpt_checked_text or default_text).strip()
        all_corrected_texts.append(final_text_page)

        gpt_lines = final_text_page.splitlines()
        lines_for_layout = []
        for i, ln in enumerate(azure_lines):
            x, y = ln["x"], ln["y"]
            text_for_line = gpt_lines[i] if i < len(gpt_lines) else ln["content"]
            lines_for_layout.append({"text": text_for_line, "x": x, "y": y})
        pages_layout.append({"page_width": cached["pw"], "page_height": cached["ph"], "unit": "pixel", "lines": lines_for_layout})

    if all_corrected_texts:
        joined_txt = "\n\n".join(all_corrected_texts)
        st.download_button("ğŸ“¥ è£œæ­£ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆTXT, ãƒšãƒ¼ã‚¸è¦‹å‡ºã—ãªã—ï¼‰",
                           data=joined_txt.encode("utf-8"), file_name="ocr_corrected.txt", mime="text/plain")
    if pages_layout:
        try:
            st.download_button("ğŸ“¥ Wordï¼ˆ.docxï¼šãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¿‘ä¼¼ï¼‰",
                               data=build_docx_from_layout(pages_layout),
                               file_name="ocr_layout.docx",
                               mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document")
        except Exception as e:
            st.warning(f"Wordå‡ºåŠ›ã«å¤±æ•—ã—ã¾ã—ãŸï¼š{e}")
