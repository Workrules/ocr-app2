import streamlit as st
import pypdfium2 as pdfium
from azure.ai.formrecognizer import DocumentAnalysisClient
from azure.core.credentials import AzureKeyCredential
import os
import io
from PIL import Image, ImageFile
import cv2
import numpy as np
import json
from openai import OpenAI
import re
import difflib
from itertools import zip_longest
from typing import List, Tuple, Dict, Any
import gc
import time

# === Wordå‡ºåŠ›é–¢é€£ ===
from docx import Document
from docx.shared import Cm, Pt, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH

# ==== ç’°å¢ƒå¤‰æ•° ====
AZURE_ENDPOINT = os.getenv("AZURE_DOCINT_ENDPOINT")
AZURE_KEY = os.getenv("AZURE_DOCINT_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MODEL_NAME = os.getenv("OCR_GPT_MODEL", "gpt-5")  # gpt-5 / gpt-5-mini ãªã©
BATCH_SIZE = max(1, int(os.getenv("OCR_BATCH_PAGES", "10")))  # 10ãƒšãƒ¼ã‚¸ãšã¤å‡¦ç†

# ==== å…±æœ‰è¾æ›¸ã®å ´æ‰€ï¼ˆç’°å¢ƒå¤‰æ•°ã§æŒ‡å®šï¼‰ ====
DICT_DIR = os.getenv("OCR_DICT_DIR", ".")
DICT_FILE = os.path.join(DICT_DIR, "ocr_char_corrections.json")
UNTRAINED_FILE = os.path.join(DICT_DIR, "untrained_confusions.json")
TRAINED_FILE = os.path.join(DICT_DIR, "trained_confusions.json")

# ==== äº‹å‰ãƒã‚§ãƒƒã‚¯ ====
if not AZURE_ENDPOINT or not AZURE_KEY:
    st.error("ç’°å¢ƒå¤‰æ•° AZURE_DOCINT_ENDPOINT ã¨ AZURE_DOCINT_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# ==== ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ– ====
client = DocumentAnalysisClient(endpoint=AZURE_ENDPOINT, credential=AzureKeyCredential(AZURE_KEY))
openai_client = OpenAI(api_key=OPENAI_API_KEY)

# ==== æ­£è¦è¡¨ç¾ ====
JP_CHAR_RE = re.compile(r"^[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF]$")

# ==== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ====
def remove_red_stamp(img_pil: Image.Image) -> Image.Image:
    img = np.array(img_pil)
    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)
    lower_red1 = np.array([0, 70, 50]); upper_red1 = np.array([10, 255, 255])
    lower_red2 = np.array([170, 70, 50]); upper_red2 = np.array([180, 255, 255])
    mask = cv2.bitwise_or(cv2.inRange(hsv, lower_red1, upper_red1),
                          cv2.inRange(hsv, lower_red2, upper_red2))
    img[mask > 0] = [255, 255, 255]
    return Image.fromarray(img)

def load_json(path: str, retries: int = 3, delay: float = 0.1) -> dict:
    for _ in range(retries):
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except FileNotFoundError:
            return {}
        except (json.JSONDecodeError, PermissionError):
            time.sleep(delay)
    return {}

def save_json(obj: dict, path: str):
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
    tmp_path = f"{path}.tmp"
    with open(tmp_path, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)
        f.flush(); os.fsync(f.fileno())
    os.replace(tmp_path, path)

def learn_charwise_with_missing(original: str, corrected: str) -> dict:
    learned = {}
    sm = difflib.SequenceMatcher(None, original, corrected)
    for tag, i1, i2, j1, j2 in sm.get_opcodes():
        if tag in ["replace", "insert"]:
            o_seg = original[i1:i2]
            c_seg = corrected[j1:j2]
            for o_char, c_char in zip_longest(o_seg, c_seg, fillvalue=""):
                if c_char and (not o_char or o_char != c_char):
                    wrong = o_char if o_char else "â–¡"
                    if JP_CHAR_RE.match(c_char) or c_char == "â–¡":
                        learned[wrong] = {"right": c_char, "count": 1}
    return learned

def update_dictionary_and_untrained(learned: dict):
    dictionary = load_json(DICT_FILE)
    for w, meta in learned.items():
        if w in dictionary:
            if dictionary[w]["right"] == meta["right"]:
                dictionary[w]["count"] += meta["count"]
            else:
                if meta["count"] > dictionary[w]["count"]:
                    dictionary[w] = meta
        else:
            dictionary[w] = meta
    save_json(dictionary, DICT_FILE)

    untrained = load_json(UNTRAINED_FILE)
    for w, meta in learned.items():
        untrained[w] = meta["right"]
    save_json(untrained, UNTRAINED_FILE)

def gpt_fix_text(text: str, dictionary: dict) -> str:
    prompt = f"""
æ¬¡ã®OCRçµæœã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«ç›´ã—ã¦ãã ã•ã„ã€‚
- æ—¥æœ¬èªã«å­˜åœ¨ã—ãªã„æ–‡å­—ã¯ã€Œâ–¡ã€ã«ã—ã¦ãã ã•ã„ã€‚
- è¾æ›¸å€™è£œã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„: {json.dumps(dictionary, ensure_ascii=False)}
- æ„å‘³ã‚’å‹æ‰‹ã«è£œå®Œã›ãšã€æœ€å°é™ã®ä¿®æ­£ã ã‘è¡Œã£ã¦ãã ã•ã„ã€‚

OCRçµæœ:
{text}
""".strip()
    try:
        resp = openai_client.responses.create(
            model=MODEL_NAME,
            input=prompt,
            text={"verbosity": "low"},
            reasoning={"effort": "minimal"}
        )
        if hasattr(resp, "output_text") and resp.output_text:
            return resp.output_text.strip()
        out_parts = []
        for item in getattr(resp, "output", []) or []:
            for part in getattr(item, "content", []) or []:
                t = getattr(part, "text", None)
                if t:
                    out_parts.append(t)
        out = "".join(out_parts).strip()
        return out or text
    except Exception as e:
        st.warning(f"GPTè£œæ­£ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰ï¼š{e}")
        return text

def is_pdf(b: bytes) -> bool:
    return len(b) >= 5 and b[:5] == b"%PDF-"

def render_pdf_selected_pages(pdf_bytes: bytes, indices_0based: List[int], dpi: int = 200) -> Tuple[List[Image.Image], List[int]]:
    imgs: List[Image.Image] = []; nums: List[int] = []
    pdf = pdfium.PdfDocument(io.BytesIO(pdf_bytes))
    scale = dpi / 72.0
    for idx in indices_0based:
        page = pdf[idx]
        pil = page.render(scale=scale).to_pil().convert("RGB")
        imgs.append(pil); nums.append(idx + 1)
    return imgs, nums

def parse_page_spec(spec: str, max_pages: int) -> List[int]:
    s = (spec or "").strip()
    if not s:
        return []
    out = set()
    parts = [p.strip() for p in s.split(",") if p.strip()]
    for p in parts:
        if "-" in p:
            a, b = p.split("-", 1)
            try:
                start = max(1, min(int(a), int(b)))
                end = min(max_pages, max(int(a), int(b)))
                for n in range(start, end + 1):
                    out.add(n - 1)
            except ValueError:
                continue
        else:
            try:
                n = int(p)
                if 1 <= n <= max_pages:
                    out.add(n - 1)
            except ValueError:
                continue
    return sorted(out)

def chunked(seq: List[int], n: int) -> List[List[int]]:
    return [seq[i:i+n] for i in range(0, len(seq), n)]

# === EMU / cm å¤‰æ›ãƒ˜ãƒ«ãƒ‘ãƒ¼ ===
EMU_PER_CM = 360000.0
def to_cm(val) -> float:
    try:
        return float(getattr(val, "cm"))
    except Exception:
        try:
            return float(val) / EMU_PER_CM
        except Exception:
            return float(val)

# === è¡Œã®ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹å–å¾— ===
def line_bbox(line_obj):
    poly = getattr(line_obj, "polygon", None) or getattr(line_obj, "bounding_polygon", None)
    if not poly:
        return 0.0, 0.0, 0.0, 0.0
    xs, ys = [], []
    for p in poly:
        x = getattr(p, "x", None); y = getattr(p, "y", None)
        if x is None and isinstance(p, dict):
            x = p.get("x", 0.0); y = p.get("y", 0.0)
        xs.append(float(x)); ys.append(float(y))
    return min(xs or [0.0]), max(xs or [0.0]), min(ys or [0.0]), max(ys or [0.0])

# === ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆæ®µã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° ===
def cluster_indents_cm(indent_values_cm, epsilon_cm=0.6):
    vals = sorted(indent_values_cm)
    clusters = []
    for v in vals:
        if not clusters:
            clusters.append({"center_cm": v, "members": [v]}); continue
        if abs(v - clusters[-1]["center_cm"]) <= epsilon_cm:
            c = clusters[-1]; c["members"].append(v)
            c["center_cm"] = sum(c["members"]) / len(c["members"])
        else:
            clusters.append({"center_cm": v, "members": [v]})
    return clusters

# === Wordç”Ÿæˆï¼ˆæ•´åˆ—/ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆæ®µã‚’åæ˜ ï¼‰ ===
def build_docx_from_layout(
    pages_layout: List[Dict[str, Any]],
    *,
    add_label: bool = False,
    center_thresh_ratio: float = 0.06,
    right_thresh_ratio: float = 0.04,
    indent_epsilon_cm: float = 0.6
) -> Tuple[bytes, List[Dict[str, Any]]]:
    doc = Document()
    section = doc.sections[0]
    section.page_width = Cm(21.0); section.page_height = Cm(29.7)
    section.left_margin = Cm(2.0); section.right_margin = Cm(2.0)
    section.top_margin = Cm(2.0); section.bottom_margin = Cm(2.0)

    page_w_cm = to_cm(section.page_width)
    left_cm = to_cm(section.left_margin); right_cm = to_cm(section.right_margin)
    content_width_cm = max(0.1, page_w_cm - left_cm - right_cm)

    style = doc.styles["Normal"]; style.font.name = "Yu Gothic"; style.font.size = Pt(11)

    all_indents_cm = []
    for page in pages_layout:
        pw = float(page.get("page_width") or 1.0)
        for ln in page.get("lines", []):
            indent_cm = (ln["x_min"] / max(pw, 1e-6)) * content_width_cm
            all_indents_cm.append(indent_cm)

    clusters = cluster_indents_cm(all_indents_cm, epsilon_cm=indent_epsilon_cm)
    clusters = sorted(clusters, key=lambda c: c["center_cm"])
    for idx, c in enumerate(clusters):
        c["index"] = idx; c["count"] = len(c["members"])

    def nearest_cluster_idx(indent_cm: float) -> int:
        if not clusters: return 0
        return min(range(len(clusters)), key=lambda i: abs(indent_cm - clusters[i]["center_cm"]))

    for page_i, page in enumerate(pages_layout, start=1):
        pw = float(page.get("page_width") or 1.0)
        ph = float(page.get("page_height") or 1.0)
        lines = sorted(page.get("lines", []), key=lambda r: (r["y"], r["x_min"]))

        center_px = pw / 2.0
        center_allow_px = pw * center_thresh_ratio
        right_allow_px = pw * right_thresh_ratio

        prev_y = None
        y_thresh = ph * 0.018

        for item in lines:
            txt = item["text"]; x_min = float(item["x_min"]); x_max = float(item["x_max"]); y = float(item["y"])
            para = doc.add_paragraph()

            line_center = (x_min + x_max) / 2.0
            align = "LEFT"
            if abs(line_center - center_px) <= center_allow_px:
                align = "CENTER"
            elif (max(0.0, pw - x_max)) <= right_allow_px:
                align = "RIGHT"

            indent_cm = (x_min / max(pw, 1e-6)) * content_width_cm
            ind_idx = nearest_cluster_idx(indent_cm)
            snapped_cm = clusters[ind_idx]["center_cm"] if clusters else indent_cm

            if add_label:
                tag = f"[{align} | ind={ind_idx}] "
                lab = para.add_run(tag); lab.font.size = Pt(8); lab.font.color.rgb = RGBColor(120, 120, 120)

            para.add_run(txt)
            para.paragraph_format.left_indent = Cm(max(0.0, min(0.95 * content_width_cm, snapped_cm)))

            if align == "CENTER":
                para.alignment = WD_ALIGN_PARAGRAPH.CENTER
            elif align == "RIGHT":
                para.alignment = WD_ALIGN_PARAGRAPH.RIGHT
            else:
                para.alignment = WD_ALIGN_PARAGRAPH.LEFT

            para.paragraph_format.space_after = Pt(2)
            if prev_y is not None and (y - prev_y) > y_thresh:
                para.paragraph_format.space_before = Pt(8)
            prev_y = y

        if page_i < len(pages_layout):
            doc.add_page_break()

    bio = io.BytesIO(); doc.save(bio); bio.seek(0)
    summary = [{"index": c["index"], "center_cm": round(c["center_cm"], 2), "count": c["count"]} for c in clusters]
    return bio.read(), summary

# ==== UI ====
st.title("ğŸ“„ rouki-ocrï¼ˆå…±æœ‰è¾æ›¸ï¼‰ - ãƒšãƒ¼ã‚¸å…ˆæŒ‡å®š / 10ãƒšãƒ¼ã‚¸ãƒãƒƒãƒ / GPTè£œæ­£ / Wordå‡ºåŠ›ï¼ˆæ•´åˆ—ï¼†æ®µï¼‰")

# è¨ºæ–­ãƒ»è¨­å®š
st.sidebar.subheader("ğŸ“– è¾æ›¸ã®å‚ç…§å…ˆ")
st.sidebar.write({
    "OCR_DICT_DIR": os.path.abspath(DICT_DIR),
    "DICT_FILE": DICT_FILE,
    "UNTRAINED_FILE": UNTRAINED_FILE,
})
dictionary_preview = load_json(DICT_FILE)
st.sidebar.json(dictionary_preview)

st.sidebar.markdown("### ğŸ”§ ç’°å¢ƒ")
st.sidebar.write({
    "AZURE_DOCINT_ENDPOINT_set": bool(AZURE_ENDPOINT),
    "AZURE_DOCINT_KEY_set": bool(AZURE_KEY),
    "OPENAI_API_KEY_set": bool(OPENAI_API_KEY),
    "MODEL": MODEL_NAME,
    "BATCH_SIZE(default)": BATCH_SIZE,
})

# --- ãƒ‡ãƒãƒƒã‚°é …ç›® ---
st.sidebar.markdown("### ğŸ›  ãƒ‡ãƒãƒƒã‚°")
skip_gpt = st.sidebar.checkbox("GPTè£œæ­£ã‚’ã‚¹ã‚­ãƒƒãƒ—", value=False)
ocr_timeout = st.sidebar.slider("OCRã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰", 10, 180, 60, step=5)
batch_size_override = st.sidebar.number_input("ãƒãƒƒãƒã‚µã‚¤ã‚ºä¸Šæ›¸ã", 1, 20, value=BATCH_SIZE)

uploaded_file = st.file_uploader("ç”»åƒã¾ãŸã¯PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=["jpg", "jpeg", "png", "pdf"])
if not uploaded_file:
    st.info("ğŸ“‚ ã“ã“ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„"); st.stop()

file_bytes = uploaded_file.read()
ImageFile.LOAD_TRUNCATED_IMAGES = True
is_input_pdf = uploaded_file.type == "application/pdf" or uploaded_file.name.lower().endswith(".pdf") or is_pdf(file_bytes)

# ==== PDFï¼šOCRå‰ã«ãƒšãƒ¼ã‚¸æŒ‡å®šãƒ•ã‚©ãƒ¼ãƒ  ====
if is_input_pdf:
    try:
        pdf_for_count = pdfium.PdfDocument(io.BytesIO(file_bytes))
        total_pages = len(pdf_for_count)
    except Exception as e:
        st.exception(e); st.stop()

    with st.form("pdf_select_form"):
        st.subheader("â–¶ OCRã™ã‚‹ãƒšãƒ¼ã‚¸ã‚’å…ˆã«é¸æŠ")
        select_mode = st.radio("é¸æŠæ–¹æ³•", options=["å…¨ãƒšãƒ¼ã‚¸", "ç¯„å›²æŒ‡å®š", "ãƒšãƒ¼ã‚¸ç•ªå·æŒ‡å®šï¼ˆä¾‹: 1,3,5-7ï¼‰"],
                               index=1 if total_pages > 1 else 0, horizontal=True)
        dpi = st.slider("ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°DPIï¼ˆé«˜ã„ã»ã©ç²¾ç´°ãƒ»é‡ã„ï¼‰", 72, 300, 200, step=4)

        center_pct = st.slider("ä¸­å¤®æƒãˆåˆ¤å®šã—ãã„å€¤ï¼ˆ%/ãƒšãƒ¼ã‚¸å¹…ï¼‰", 2, 12, 6, step=1)
        indent_eps = st.slider("ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆæ®µã¾ã¨ã‚ ã—ãã„å€¤ï¼ˆcmï¼‰", 0.2, 1.5, 0.6, step=1/10)
        show_labels = st.checkbox("Wordã«ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãƒ©ãƒ™ãƒ«ã‚’æ›¸ãï¼ˆ[CENTER | ind=n]ï¼‰", value=False)

        if select_mode == "ç¯„å›²æŒ‡å®š" and total_pages > 1:
            start, end = st.slider("å‡¦ç†ã™ã‚‹ãƒšãƒ¼ã‚¸ç¯„å›²ï¼ˆ1å§‹ã¾ã‚Šï¼‰", 1, total_pages, (1, min(total_pages, 5)))
            chosen_indices = list(range(start - 1, end))
        elif select_mode == "ãƒšãƒ¼ã‚¸ç•ªå·æŒ‡å®šï¼ˆä¾‹: 1,3,5-7ï¼‰":
            spec = st.text_input("ãƒšãƒ¼ã‚¸ç•ªå·ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã€ç¯„å›²ã¯ãƒã‚¤ãƒ•ãƒ³ï¼‰",
                                 value="1-3" if total_pages >= 3 else "1")
            chosen_indices = parse_page_spec(spec, total_pages)
            if not chosen_indices:
                st.info("æœ‰åŠ¹ãªãƒšãƒ¼ã‚¸ç•ªå·ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ä¾‹: 1,3,5-7")
        else:
            chosen_indices = list(range(total_pages))

        submitted = st.form_submit_button("ã“ã®ãƒšãƒ¼ã‚¸ã ã‘OCRã‚’å®Ÿè¡Œ")

    if not submitted or not chosen_indices:
        st.stop()

    EFFECTIVE_BATCH = int(batch_size_override) if batch_size_override else BATCH_SIZE

    total_to_process = len(chosen_indices)
    progress = st.progress(0.0); status = st.empty()
    all_corrected_texts: List[str] = []
    pages_layout: List[Dict[str, Any]] = []
    done = 0

    for batch_no, batch_indices in enumerate(chunked(chosen_indices, EFFECTIVE_BATCH), start=1):
        status.info(f"ğŸ”„ ãƒãƒƒãƒ {batch_no} / {((total_to_process - 1) // EFFECTIVE_BATCH) + 1} ã‚’å‡¦ç†ä¸­ï¼ˆãƒšãƒ¼ã‚¸: {', '.join(str(i+1) for i in batch_indices)}ï¼‰")
        try:
            pages, page_numbers = render_pdf_selected_pages(file_bytes, batch_indices, dpi=dpi)
        except Exception as e:
            st.exception(e); st.stop()

        for page_img, page_num in zip(pages, page_numbers):
            st.write(f"## ãƒšãƒ¼ã‚¸ {page_num}")
            clean_img = remove_red_stamp(page_img)

            buf = io.BytesIO(); clean_img.save(buf, format="PNG"); buf.seek(0)

            with st.spinner("OCRã‚’å®Ÿè¡Œä¸­..."):
                t0 = time.perf_counter()
                try:
                    poller = client.begin_analyze_document("prebuilt-read", document=buf)
                    result = poller.result(timeout=float(ocr_timeout))
                except Exception as e:
                    st.error(f"OCRãŒ{ocr_timeout}ç§’ä»¥å†…ã«å®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸ / å¤±æ•—ã—ã¾ã—ãŸï¼š{e}")
                    st.caption(f"OCRå®Ÿè¡Œæ™‚é–“: {time.perf_counter() - t0:.1f}s")
                    continue
                st.caption(f"OCRå®Ÿè¡Œæ™‚é–“: {time.perf_counter() - t0:.1f}s")

            doc_page = result.pages[0] if getattr(result, "pages", None) else None
            if not doc_page:
                st.warning("OCRçµæœã«ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                done += 1; progress.progress(done / total_to_process); continue

            azure_lines = getattr(doc_page, "lines", []) or []
            default_text = "\n".join([line.content for line in azure_lines])

            dictionary = load_json(DICT_FILE)
            gpt_checked_text = default_text if skip_gpt else gpt_fix_text(default_text, dictionary)

            tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“„ å…ƒãƒ•ã‚¡ã‚¤ãƒ«", "ğŸ–¨ï¸ OCRãƒ†ã‚­ã‚¹ãƒˆ", "ğŸ¤– GPTè£œæ­£", "âœï¸ æ‰‹ä½œæ¥­ä¿®æ­£"])
            with tab1:
                st.image(clean_img, caption=f"å…ƒãƒ•ã‚¡ã‚¤ãƒ« (ãƒšãƒ¼ã‚¸ {page_num})", use_container_width=True)
            with tab2:
                st.text_area(f"OCRãƒ†ã‚­ã‚¹ãƒˆï¼ˆãƒšãƒ¼ã‚¸ {page_num}ï¼‰", default_text, height=320, key=f"ocr_{page_num}")
            with tab3:
                st.text_area(f"GPTè£œæ­£ï¼ˆãƒšãƒ¼ã‚¸ {page_num}ï¼‰", gpt_checked_text, height=320, key=f"gpt_{page_num}")
            with tab4:
                corrected_text = st.text_area(f"æ‰‹ä½œæ¥­ä¿®æ­£ï¼ˆãƒšãƒ¼ã‚¸ {page_num}ï¼‰", gpt_checked_text, height=320, key=f"edit_{page_num}")
                if st.button(f"ä¿®æ­£ã‚’ä¿å­˜ (ãƒšãƒ¼ã‚¸ {page_num})", key=f"save_{page_num}"):
                    learned = learn_charwise_with_missing(default_text, corrected_text)
                    if learned:
                        update_dictionary_and_untrained(learned)
                        st.success(f"è¾æ›¸ã¨å­¦ç¿’å€™è£œã« {len(learned)} ä»¶ã‚’è¿½åŠ ã—ã¾ã—ãŸï¼")
                    else:
                        st.info("ä¿®æ­£ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                    st.rerun()

            final_text_page = (corrected_text or gpt_checked_text).strip()
            all_corrected_texts.append(final_text_page)

            gpt_lines = [ln for ln in (corrected_text or gpt_checked_text).splitlines()]
            lines_for_layout = []
            for i, ln in enumerate(azure_lines):
                x_min, x_max, y_min, y_max = line_bbox(ln)
                text_for_line = gpt_lines[i] if i < len(gpt_lines) else ln.content
                lines_for_layout.append({"text": text_for_line, "x_min": x_min, "x_max": x_max, "y": y_min})

            page_layout_info = {
                "page_width": getattr(doc_page, "width", None) or 1.0,
                "page_height": getattr(doc_page, "height", None) or 1.0,
                "unit": getattr(doc_page, "unit", None) or "pixel",
                "lines": lines_for_layout
            }
            pages_layout.append(page_layout_info)

            done += 1; progress.progress(done / total_to_process)

        del pages, page_numbers
        gc.collect()

    status.success("âœ… ã™ã¹ã¦ã®ãƒšãƒ¼ã‚¸ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    if all_corrected_texts:
        joined_txt = "\n\n".join(all_corrected_texts)
        st.download_button("ğŸ“¥ è£œæ­£ãƒ†ã‚­ã‚¹ãƒˆï¼ˆTXT, è¦‹å‡ºã—ãªã—ï¼‰", data=joined_txt.encode("utf-8"),
                           file_name="ocr_corrected.txt", mime="text/plain")

    if pages_layout:
        try:
            docx_bytes, indent_summary = build_docx_from_layout(
                pages_layout,
                add_label=show_labels,
                center_thresh_ratio=center_pct / 100.0,
                indent_epsilon_cm=float(indent_eps)
            )
            st.download_button("ğŸ“¥ Wordï¼ˆ.docxï¼šæ•´åˆ—/æ®µã‚’åæ˜ ï¼‰", data=docx_bytes,
                               file_name="ocr_layout.docx",
                               mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document")
            if indent_summary:
                st.markdown("#### æ¤œå‡ºã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆæ®µ")
                for s in indent_summary:
                    st.write(f"- æ®µ {s['index']} : ä¸­å¿ƒ {s['center_cm']} cmï¼ˆ{s['count']} è¡Œï¼‰")
        except Exception as e:
            st.warning(f"Wordå‡ºåŠ›ã«å¤±æ•—ã—ã¾ã—ãŸï¼š{e}")

else:
    # ==== ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ï¼š1ãƒšãƒ¼ã‚¸å‡¦ç† ====
    try:
        try:
            img = Image.open(io.BytesIO(file_bytes)).convert("RGB")
        except Exception:
            arr = np.frombuffer(file_bytes, dtype=np.uint8)
            bgr = cv2.imdecode(arr, cv2.IMREAD_COLOR)
            if bgr is None:
                raise ValueError("ç”»åƒã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆJPG/PNG/PDFã®ã¿å¯¾å¿œï¼‰ã€‚")
            img = Image.fromarray(cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB))
        pages = [img]; page_numbers = [1]
    except Exception as e:
        st.exception(e); st.stop()

    center_pct = 6; indent_eps = 0.6; show_labels = False
    all_corrected_texts: List[str] = []; pages_layout: List[Dict[str, Any]] = []

    for page_img, page_num in zip(pages, page_numbers):
        st.write(f"## ãƒšãƒ¼ã‚¸ {page_num}")
        clean_img = remove_red_stamp(page_img)

        buf = io.BytesIO(); clean_img.save(buf, format="PNG"); buf.seek(0)

        with st.spinner("OCRã‚’å®Ÿè¡Œä¸­..."):
            t0 = time.perf_counter()
            try:
                poller = client.begin_analyze_document("prebuilt-read", document=buf)
                result = poller.result(timeout=float(ocr_timeout))
            except Exception as e:
                st.error(f"OCRãŒ{ocr_timeout}ç§’ä»¥å†…ã«å®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸ / å¤±æ•—ã—ã¾ã—ãŸï¼š{e}")
                st.caption(f"OCRå®Ÿè¡Œæ™‚é–“: {time.perf_counter() - t0:.1f}s")
                continue
            st.caption(f"OCRå®Ÿè¡Œæ™‚é–“: {time.perf_counter() - t0:.1f}s")

        doc_page = result.pages[0] if getattr(result, "pages", None) else None
        if not doc_page:
            st.warning("OCRçµæœã«ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"); continue

        azure_lines = getattr(doc_page, "lines", []) or []
        default_text = "\n".join([line.content for line in azure_lines])

        dictionary = load_json(DICT_FILE)
        gpt_checked_text = default_text if skip_gpt else gpt_fix_text(default_text, dictionary)

        tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“„ å…ƒãƒ•ã‚¡ã‚¤ãƒ«", "ğŸ–¨ï¸ OCRãƒ†ã‚­ã‚¹ãƒˆ", "ğŸ¤– GPTè£œæ­£", "âœï¸ æ‰‹ä½œæ¥­ä¿®æ­£"])
        with tab1:
            st.image(clean_img, caption=f"å…ƒãƒ•ã‚¡ã‚¤ãƒ« (ãƒšãƒ¼ã‚¸ {page_num})", use_container_width=True)
        with tab2:
            st.text_area(f"OCRãƒ†ã‚­ã‚¹ãƒˆï¼ˆãƒšãƒ¼ã‚¸ {page_num}ï¼‰", default_text, height=320, key=f"ocr_{page_num}")
        with tab3:
            st.text_area(f"GPTè£œæ­£ï¼ˆãƒšãƒ¼ã‚¸ {page_num}ï¼‰", gpt_checked_text, height=320, key=f"gpt_{page_num}")
        with tab4:
            corrected_text = st.text_area(f"æ‰‹ä½œæ¥­ä¿®æ­£ï¼ˆãƒšãƒ¼ã‚¸ {page_num}ï¼‰", gpt_checked_text, height=320, key=f"edit_{page_num}")
            if st.button(f"ä¿®æ­£ã‚’ä¿å­˜ (ãƒšãƒ¼ã‚¸ {page_num})", key=f"save_{page_num}"):
                learned = learn_charwise_with_missing(default_text, corrected_text)
                if learned:
                    update_dictionary_and_untrained(learned)
                    st.success(f"è¾æ›¸ã¨å­¦ç¿’å€™è£œã« {len(learned)} ä»¶ã‚’è¿½åŠ ã—ã¾ã—ãŸï¼")
                else:
                    st.info("ä¿®æ­£ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                st.rerun()

        final_text_page = (corrected_text or gpt_checked_text).strip()
        all_corrected_texts.append(final_text_page)

        gpt_lines = [ln for ln in (corrected_text or gpt_checked_text).splitlines()]
        lines_for_layout = []
        for i, ln in enumerate(azure_lines):
            x_min, x_max, y_min, y_max = line_bbox(ln)
            text_for_line = gpt_lines[i] if i < len(gpt_lines) else ln.content
            lines_for_layout.append({"text": text_for_line, "x_min": x_min, "x_max": x_max, "y": y_min})

        page_layout_info = {
            "page_width": getattr(doc_page, "width", None) or 1.0,
            "page_height": getattr(doc_page, "height", None) or 1.0,
            "unit": getattr(doc_page, "unit", None) or "pixel",
            "lines": lines_for_layout
        }
        pages_layout.append(page_layout_info)

    if all_corrected_texts:
        joined_txt = "\n\n".join(all_corrected_texts)
        st.download_button("ğŸ“¥ è£œæ­£ãƒ†ã‚­ã‚¹ãƒˆï¼ˆTXT, è¦‹å‡ºã—ãªã—ï¼‰", data=joined_txt.encode("utf-8"),
                           file_name="ocr_corrected.txt", mime="text/plain")

    if pages_layout:
        try:
            docx_bytes, indent_summary = build_docx_from_layout(
                pages_layout, add_label=show_labels, center_thresh_ratio=center_pct/100.0, indent_epsilon_cm=float(indent_eps)
            )
            st.download_button("ğŸ“¥ Wordï¼ˆ.docxï¼šæ•´åˆ—/æ®µã‚’åæ˜ ï¼‰", data=docx_bytes,
                               file_name="ocr_layout.docx",
                               mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document")
            if indent_summary:
                st.markdown("#### æ¤œå‡ºã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆæ®µ")
                for s in indent_summary:
                    st.write(f"- æ®µ {s['index']} : ä¸­å¿ƒ {s['center_cm']} cmï¼ˆ{s['count']} è¡Œï¼‰")
        except Exception as e:
            st.warning(f"Wordå‡ºåŠ›ã«å¤±æ•—ã—ã¾ã—ãŸï¼š{e}")
